{
    "seed": 0,
    "data": {
        "name": "wikitq",
        "data_path": "wikitablequestions",
        "use_title": true,
        "use_table_caption": true,
        "pad_token_id": 0,
        "masked_output_file": "datasets/scigen/masked_desc.pkl",
        "maskable_words_file": "datasets/scigen/maskable_words.pkl",
        "config_name": null,
        "num_classes": null,
        "decompose_table": false,
        "topk_cells": 15,
        "soft_decomposition_data_path": "datasets/wiki_tq"
    },
    "logging": {
        "log_dir": "logs/table_question_answering_tapex_t5_baseline",
        "project": "table_question_answering",
        "name": "table_question_answering_tapex_t5_baseline",
        "version": "table_question_answering_tapex_t5_baseline",
        "save_dir": "",
        "log_model": true,
        "checkpoint_dir": "logs/table_question_answering_tapex_t5_baseline/checkpoints"
    },

    "metrics": {
        "bert_score_model": "bert-base-uncased"
    },

    "model": {
        "model_name": "t5",
        "model_path": "SivilTaram/tapex-t5-large-finetuned-wtq",
        "soft_decomposition_model": "bert-base",
        "soft_decomposition_model_path": "bert-base-uncased",
        "checkpoint": null,
        "use_table": true,
        "pretraining_task": "masked_language_modelling",
        "num_return_sequences": 1,
        "num_beams": 3,
        "use_pretrained": true,
        "use_position_ids": false,
        "type": "encoder-decoder",
        "num_encoders": 1,
        "quantize": false,
        "peft": false
    },

    "system": {
        "num_workers": 4,
        "gpus": 2
    },

    "tokenizer": {
        "tokenizer_path": "SivilTaram/tapex-t5-large-finetuned-wtq",
        "soft_decomposition_tokenizer_path": "bert-base-uncased",
        "local_files_only": false,
        "add_special_tokens": true,
        "padding": "max_length",
        "truncation": true,
        "max_length": 960,
        "input_max_length": 960,
        "output_max_length": 64,
        "return_tensors": "pt",
        "return_token_type_ids": true,
        "return_attention_mask": true,
        "special_table_tok": false,
        "padding_side": "right",
        "use_row_col_ids": false
    },

    "training": {
        "epochs": 30,
        "train_batch_size": 3,
        "test_batch_size": 3,
        "training_type": "table_question_answering",
        "use_title": false,
        "downstream": true,
        "accumulate_grad_batches": 2,
        "sync_batchnorm": true,
        "gradient_clip_val": 1,
        "lr": 1e-5,
        "scheduler": false,
        "grad_accumulation": true,
        "accumulation_steps": 1,
        "criterion": "cross_entropy",
        "sharded": false
    }
}