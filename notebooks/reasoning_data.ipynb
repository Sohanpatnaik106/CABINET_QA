{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from utils import process_config\n",
    "from src import T5ModelForTableReasoning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from data import WikiTQReasoningDataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"configs/wiki_tq_reasoning/t5.json\") as f:\n",
    "    config = json.load(f)\n",
    "config = process_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitablequestions\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ModelForTableReasoning(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"logs/table_question_reasoning_t5_3b_bootstrapping_baseline_loss_calc_change/checkpoints/epoch=80.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pretrained = T5ModelForTableReasoning(config)\n",
    "model_pretrained.to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = dataset[idx][\"question\"]\n",
    "answer = \", \".join(dataset[idx][\"answers\"]).lower()\n",
    "table_column_names = dataset[idx][\"table\"][\"header\"]\n",
    "table_content_values = dataset[idx][\"table\"][\"rows\"]\n",
    "\n",
    "table_df = pd.DataFrame.from_dict({str(col).lower(): [str(table_content_values[j][i]).lower() for j in range(len(table_content_values))] for i, col in enumerate(table_column_names)})\n",
    "\n",
    "table =  \"[HEADER] \" + \" | \".join(table_column_names)\n",
    "for row_id, row in enumerate(table_content_values):\n",
    "    table += f\" [ROW] {row_id}: \" + \" | \".join(row) \n",
    "\n",
    "input_text = f\"Question: {question} Answer: {answer}. \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "\n",
    "display(table_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input  = tokenizer(input_text, table, add_special_tokens = config.tokenizer.add_special_tokens,\n",
    "                            padding = config.tokenizer.padding, truncation = config.tokenizer.truncation, \n",
    "                            max_length = config.tokenizer.max_length, return_tensors = config.tokenizer.return_tensors,\n",
    "                            return_token_type_ids = config.tokenizer.return_token_type_ids,\n",
    "                            return_attention_mask = config.tokenizer.return_attention_mask)\n",
    "\n",
    "input_ids = tokenized_input[\"input_ids\"]\n",
    "attention_mask = tokenized_input[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ids = model.model.generate(input_ids = input_ids.to(\"cuda:0\"), attention_mask = attention_mask.to(\"cuda:0\"), \n",
    "                                     max_new_tokens = config.tokenizer.output_max_length, num_beams = 3, early_stopping = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_reason = tokenizer.decode(predicted_ids.squeeze(), skip_special_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ids_pretrained = model_pretrained.model.generate(input_ids = input_ids.to(\"cuda:1\"), attention_mask = attention_mask.to(\"cuda:1\"), \n",
    "                                     max_new_tokens = config.tokenizer.output_max_length, num_beams = 3, early_stopping = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_reason_pretrained = tokenizer.decode(predicted_ids_pretrained.squeeze(), skip_special_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_reason_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_dataset = pd.read_csv(config.data.data_path)\n",
    "train_dataset = WikiTQReasoningDataset(dataset = reasoning_dataset, config= config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_input_ids, reason_attention_mask, _, reason_output_ids, reason_labels = train_dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tokenizer.decode(reason_labels[reason_labels != -100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ids = tokenizer(x, add_special_tokens = config.tokenizer.add_special_tokens,\n",
    "                            padding = config.tokenizer.padding, truncation = config.tokenizer.truncation, \n",
    "                            max_length = config.tokenizer.max_length, return_tensors = config.tokenizer.return_tensors,\n",
    "                            return_token_type_ids = config.tokenizer.return_token_type_ids,\n",
    "                            return_attention_mask = config.tokenizer.return_attention_mask)[\"input_ids\"].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"bos_token\" not in list(tokenizer.special_tokens_map.keys()):\n",
    "    tokenizer.add_special_tokens({\"bos_token\": tokenizer.special_tokens_map[\"eos_token\"]})\n",
    "\n",
    "if \"pad_token\" not in list(tokenizer.special_tokens_map.keys()):\n",
    "    tokenizer.add_special_tokens({\"pad_token\": tokenizer.special_tokens_map[\"eos_token\"]})\n",
    "\n",
    "if \"sep_token\" not in list(tokenizer.special_tokens_map.keys()):\n",
    "    tokenizer.add_special_tokens({\"sep_token\": tokenizer.special_tokens_map[\"eos_token\"]})\n",
    "\n",
    "if \"mask_token\" not in list(tokenizer.special_tokens_map.keys()):\n",
    "    tokenizer.add_special_tokens({\"mask_token\": tokenizer.special_tokens_map[\"eos_token\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ids[x_ids == tokenizer.convert_tokens_to_ids(tokenizer.special_tokens_map[\"pad_token\"])] = -100\n",
    "# x_ids[x_ids == tokenizer.convert_tokens_to_ids(tokenizer.special_tokens_map[\"sep_token\"])] = -100\n",
    "# x_ids[x_ids == tokenizer.convert_tokens_to_ids(tokenizer.special_tokens_map[\"bos_token\"])] = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(x_ids[x_ids != -100])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Reasons on WikiTQ dataset using T5-3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "from src import T5ModelForTableReasoning\n",
    "from utils import process_config\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiTQReasoningDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset, config, data_type = \"train\"):\n",
    "        super(WikiTQReasoningDataset, self).__init__()\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.config = config\n",
    "        self.data_type = data_type\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer.tokenizer_path, local_files_only = self.config.tokenizer.local_files_only,\n",
    "                                                       padding_side = self.config.tokenizer.padding_side)\n",
    "\n",
    "        \n",
    "        if \"bos_token\" not in list(self.tokenizer.special_tokens_map.keys()):\n",
    "            self.tokenizer.add_special_tokens({\"bos_token\": \"<s>\"})\n",
    "\n",
    "        if \"pad_token\" not in list(self.tokenizer.special_tokens_map.keys()):\n",
    "            self.tokenizer.add_special_tokens({\"pad_token\": self.tokenizer.special_tokens_map[\"eos_token\"]})\n",
    "\n",
    "        if \"sep_token\" not in list(self.tokenizer.special_tokens_map.keys()):\n",
    "            self.tokenizer.add_special_tokens({\"sep_token\": self.tokenizer.special_tokens_map[\"eos_token\"]})\n",
    "\n",
    "        if \"mask_token\" not in list(self.tokenizer.special_tokens_map.keys()):\n",
    "            self.tokenizer.add_special_tokens({\"mask_token\": self.tokenizer.special_tokens_map[\"eos_token\"]})\n",
    "\n",
    "        self.text_input, self.table, self.text_output = self._process_dataset()\n",
    "\n",
    "\n",
    "    def _tokenize(self, text_input, table = None, max_length = 512, text_output = None):\n",
    "\n",
    "        if text_output is not None:\n",
    "            if self.config.tokenizer.special_table_tok:\n",
    "                raise NotImplementedError\n",
    "            else:\n",
    "                if table is not None:\n",
    "                    table = table + f\" {self.tokenizer.special_tokens_map['sep_token']} \" + text_output\n",
    "                else:\n",
    "                    text_input = text_input + f\" {self.tokenizer.special_tokens_map['sep_token']} \" + text_output\n",
    "            # text_input = text_input + f\" {self.tokenizer.special_tokens_map['sep_token']} \" + text_output\n",
    "\n",
    "        if self.config.tokenizer.special_table_tok:\n",
    "            if table is not None:\n",
    "                return self.tokenizer(table, text_input, add_special_tokens = self.config.tokenizer.add_special_tokens,\n",
    "                            padding = self.config.tokenizer.padding, truncation = self.config.tokenizer.truncation, \n",
    "                            max_length = max_length, return_tensors = self.config.tokenizer.return_tensors,\n",
    "                            return_token_type_ids = self.config.tokenizer.return_token_type_ids,\n",
    "                            return_attention_mask = self.config.tokenizer.return_attention_mask)\n",
    "            else: \n",
    "                return self.tokenizer(answer = text_input, add_special_tokens = self.config.tokenizer.add_special_tokens,\n",
    "                            padding = self.config.tokenizer.padding, truncation = self.config.tokenizer.truncation, \n",
    "                            max_length = max_length, return_tensors = self.config.tokenizer.return_tensors,\n",
    "                            return_token_type_ids = self.config.tokenizer.return_token_type_ids,\n",
    "                            return_attention_mask = self.config.tokenizer.return_attention_mask)\n",
    "        else:\n",
    "            if table is not None:\n",
    "                return self.tokenizer(text_input, table, add_special_tokens = self.config.tokenizer.add_special_tokens,\n",
    "                            padding = self.config.tokenizer.padding, truncation = self.config.tokenizer.truncation, \n",
    "                            max_length = max_length, return_tensors = self.config.tokenizer.return_tensors,\n",
    "                            return_token_type_ids = self.config.tokenizer.return_token_type_ids,\n",
    "                            return_attention_mask = self.config.tokenizer.return_attention_mask)\n",
    "            else:\n",
    "                return self.tokenizer(text_input, add_special_tokens = self.config.tokenizer.add_special_tokens,\n",
    "                            padding = self.config.tokenizer.padding, truncation = self.config.tokenizer.truncation, \n",
    "                            max_length = max_length, return_tensors = self.config.tokenizer.return_tensors,\n",
    "                            return_token_type_ids = self.config.tokenizer.return_token_type_ids,\n",
    "                            return_attention_mask = self.config.tokenizer.return_attention_mask)\n",
    "\n",
    "\n",
    "    def _process_one_sample(self, data, idx = None):\n",
    "\n",
    "        question = data[\"question\"]\n",
    "        table_column_names = data[\"table\"][\"header\"]\n",
    "        table_content_values = data[\"table\"][\"rows\"]\n",
    "\n",
    "        answer = data[\"answers\"]\n",
    "        answer_list = answers = [str(a).lower() for a in data[\"answers\"]]\n",
    "        answer = f\", \".join(answer).lower()\n",
    "\n",
    "        output_text = \"\"\n",
    "        input_text = f\"Question: {question} Answer: {answer}. \"\n",
    "\n",
    "\n",
    "        if self.config.tokenizer.special_table_tok:\n",
    "\n",
    "            table = pd.DataFrame.from_dict({str(col).lower(): [str(table_content_values[j][i]).lower() for j in range(len(table_content_values))] for i, col in enumerate(table_column_names)})\n",
    "\n",
    "            if self.config.data.decompose_table:\n",
    "                relevant_rows, relevant_columns = self._decompose_table(question, answer_list, table)\n",
    "                \n",
    "                if self.config.training.training_type != \"table_decomposition\":\n",
    "                    \n",
    "                    if len(relevant_rows) > 0:\n",
    "                        table = table.iloc[relevant_rows]\n",
    "                    \n",
    "                    elif len(relevant_columns) > 0:\n",
    "                        table = table[relevant_columns]\n",
    "                else:\n",
    "                    if len(relevant_rows) > 0:\n",
    "                        table_output = table.iloc[relevant_rows]\n",
    "                    \n",
    "                    elif len(relevant_columns) > 0:\n",
    "                        table_output = table[relevant_columns]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            if self.config.data.decompose_table:\n",
    "                table = pd.DataFrame.from_dict({str(col).lower(): [str(table_content_values[j][i]).lower() for j in range(len(table_content_values))] for i, col in enumerate(table_column_names)})\n",
    "                relevant_rows, relevant_columns = self._decompose_table(question, answer_list, table)\n",
    "                \n",
    "                if self.config.training.training_type != \"table_decomposition\":\n",
    "                    if len(relevant_rows) > 0:\n",
    "                        table = table.iloc[relevant_rows]\n",
    "                    \n",
    "                    elif len(relevant_columns) > 0:\n",
    "                        table = table[relevant_columns]\n",
    "\n",
    "                    table_column_names = table.columns.tolist()\n",
    "                    table_content_values = table.values.tolist()\n",
    "\n",
    "                else:\n",
    "                    if len(relevant_rows) > 0:\n",
    "                        table_output = table.iloc[relevant_rows]\n",
    "                    \n",
    "                    elif len(relevant_columns) > 0:\n",
    "                        table_output = table[relevant_columns]\n",
    "\n",
    "\n",
    "            table = \"[HEADER] \" + \" | \".join(table_column_names)\n",
    "            for row_id, row in enumerate(table_content_values):\n",
    "                table += f\" [ROW] {row_id}: \" + \" | \".join(row) \n",
    "\n",
    "            if self.config.training.training_type == \"table_decomposition\":\n",
    "                table_column_names_output = table_output.columns.tolist()\n",
    "                table_content_values_output = table_output.values.tolist()\n",
    "\n",
    "                table_output = \"[HEADER] \" + \" | \".join(table_column_names_output)\n",
    "                for row_id, row in enumerate(table_content_values_output):\n",
    "                    table_output += f\" [ROW] {row_id}: \" + \" | \".join(row)\n",
    "\n",
    "        if self.config.training.training_type == \"table_decomposition\":\n",
    "            return question, table, table_output\n",
    "        else:\n",
    "            return input_text, table, output_text\n",
    "\n",
    "    \n",
    "\n",
    "    def _process_dataset(self):\n",
    "\n",
    "        processed_data = []\n",
    "        for i, data in tqdm(enumerate(self.dataset[self.data_type]), position = 0, leave = True, total = len(self.dataset[self.data_type])):\n",
    "            processed_data.append(self._process_one_sample(data, i))\n",
    "\n",
    "\n",
    "        text_input = [x[0] for x in processed_data]\n",
    "        table = [x[1] for x in processed_data]\n",
    "        text_output = [x[2] for x in processed_data]\n",
    "\n",
    "        return text_input, table, text_output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_input)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        tokenized_input = self._tokenize(self.text_input[index], self.table[index], max_length = self.config.tokenizer.input_max_length)\n",
    "        return tokenized_input[\"input_ids\"].squeeze(), tokenized_input[\"attention_mask\"].squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitablequestions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"configs/wiki_tq_reasoning/t5.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "config = process_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = WikiTQReasoningDataset(dataset = dataset, config = config, data_type = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size = 8, shuffle = False, num_workers = config.system.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_generations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ModelForTableReasoning(config)\n",
    "model.load_state_dict(torch.load(\"logs/table_question_reasoning_t5_3b_bootstrapping_baseline_loss_calc_change/checkpoints/epoch=80.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda:6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in tqdm(enumerate(train_dataloader), position = 0, leave = True, total = len(train_dataloader)):\n",
    "\n",
    "    input_ids, attention_mask = batch\n",
    "    predicted_ids = model.model.generate(input_ids = input_ids.to(\"cuda:6\"), attention_mask = attention_mask.to(\"cuda:6\"), \n",
    "                                     max_new_tokens = config.tokenizer.output_max_length, num_beams = 3, early_stopping = True).detach().cpu()\n",
    "\n",
    "    batch_predicted_reason = train_dataset.tokenizer.batch_decode(predicted_ids, skip_special_tokens = True)\n",
    "\n",
    "    reason_generations.extend(batch_predicted_reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reason_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_generations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_generations[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"datasets/test_wiki_tq_reason.pkl\", \"wb\") as f:\n",
    "    pickle.dump(reason_generations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"datasets/wiki_tq_reason.pkl\", \"rb\") as f:\n",
    "    reason_generations = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "gold_df = pd.read_csv(\"datasets/WikiTQReasoningData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(gold_df)):\n",
    "    idx = gold_df[\"id\"][i]\n",
    "    reason_generations[idx] = gold_df[\"reason\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"datasets/wiki_tq_reason.pkl\", \"wb\") as f:\n",
    "    pickle.dump(reason_generations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"datasets/wiki_tq_reason.pkl\", \"rb\") as f:\n",
    "    reason_generations = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"wikitablequestions\")[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = train_dataset[idx][\"question\"]\n",
    "answer = \", \".join(train_dataset[idx][\"answers\"]).lower()\n",
    "table_column_names = train_dataset[idx][\"table\"][\"header\"]\n",
    "table_content_values = train_dataset[idx][\"table\"][\"rows\"]\n",
    "\n",
    "table = pd.DataFrame.from_dict({str(col).lower(): [str(table_content_values[j][i]).lower() for j in range(len(table_content_values))] for i, col in enumerate(table_column_names)})\n",
    "\n",
    "reason = reason_generations[idx]\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "print(f\"reason: {reason}\")\n",
    "\n",
    "display(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Reasons on WikiTQ dataset using T5-3b trained without answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "from src import T5ModelForTableReasoning\n",
    "from utils import process_config\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiTQReasoningWithoutAnswerDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset, config, data_type = \"train\"):\n",
    "        super(WikiTQReasoningWithoutAnswerDataset, self).__init__()\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.config = config\n",
    "        self.data_type = data_type\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer.tokenizer_path, local_files_only = self.config.tokenizer.local_files_only,\n",
    "                                                       padding_side = self.config.tokenizer.padding_side)\n",
    "\n",
    "        \n",
    "        if \"bos_token\" not in list(self.tokenizer.special_tokens_map.keys()):\n",
    "            self.tokenizer.add_special_tokens({\"bos_token\": \"<s>\"})\n",
    "\n",
    "        if \"pad_token\" not in list(self.tokenizer.special_tokens_map.keys()):\n",
    "            self.tokenizer.add_special_tokens({\"pad_token\": self.tokenizer.special_tokens_map[\"eos_token\"]})\n",
    "\n",
    "        if \"sep_token\" not in list(self.tokenizer.special_tokens_map.keys()):\n",
    "            self.tokenizer.add_special_tokens({\"sep_token\": self.tokenizer.special_tokens_map[\"eos_token\"]})\n",
    "\n",
    "        if \"mask_token\" not in list(self.tokenizer.special_tokens_map.keys()):\n",
    "            self.tokenizer.add_special_tokens({\"mask_token\": self.tokenizer.special_tokens_map[\"eos_token\"]})\n",
    "\n",
    "        self.text_input, self.table, self.text_output = self._process_dataset()\n",
    "\n",
    "\n",
    "    def _tokenize(self, text_input, table = None, max_length = 512, text_output = None):\n",
    "\n",
    "        if text_output is not None:\n",
    "            if self.config.tokenizer.special_table_tok:\n",
    "                raise NotImplementedError\n",
    "            else:\n",
    "                if table is not None:\n",
    "                    table = table + f\" {self.tokenizer.special_tokens_map['sep_token']} \" + text_output\n",
    "                else:\n",
    "                    text_input = text_input + f\" {self.tokenizer.special_tokens_map['sep_token']} \" + text_output\n",
    "            # text_input = text_input + f\" {self.tokenizer.special_tokens_map['sep_token']} \" + text_output\n",
    "\n",
    "        if self.config.tokenizer.special_table_tok:\n",
    "            if table is not None:\n",
    "                return self.tokenizer(table, text_input, add_special_tokens = self.config.tokenizer.add_special_tokens,\n",
    "                            padding = self.config.tokenizer.padding, truncation = self.config.tokenizer.truncation, \n",
    "                            max_length = max_length, return_tensors = self.config.tokenizer.return_tensors,\n",
    "                            return_token_type_ids = self.config.tokenizer.return_token_type_ids,\n",
    "                            return_attention_mask = self.config.tokenizer.return_attention_mask)\n",
    "            else: \n",
    "                return self.tokenizer(answer = text_input, add_special_tokens = self.config.tokenizer.add_special_tokens,\n",
    "                            padding = self.config.tokenizer.padding, truncation = self.config.tokenizer.truncation, \n",
    "                            max_length = max_length, return_tensors = self.config.tokenizer.return_tensors,\n",
    "                            return_token_type_ids = self.config.tokenizer.return_token_type_ids,\n",
    "                            return_attention_mask = self.config.tokenizer.return_attention_mask)\n",
    "        else:\n",
    "            if table is not None:\n",
    "                return self.tokenizer(text_input, table, add_special_tokens = self.config.tokenizer.add_special_tokens,\n",
    "                            padding = self.config.tokenizer.padding, truncation = self.config.tokenizer.truncation, \n",
    "                            max_length = max_length, return_tensors = self.config.tokenizer.return_tensors,\n",
    "                            return_token_type_ids = self.config.tokenizer.return_token_type_ids,\n",
    "                            return_attention_mask = self.config.tokenizer.return_attention_mask)\n",
    "            else:\n",
    "                return self.tokenizer(text_input, add_special_tokens = self.config.tokenizer.add_special_tokens,\n",
    "                            padding = self.config.tokenizer.padding, truncation = self.config.tokenizer.truncation, \n",
    "                            max_length = max_length, return_tensors = self.config.tokenizer.return_tensors,\n",
    "                            return_token_type_ids = self.config.tokenizer.return_token_type_ids,\n",
    "                            return_attention_mask = self.config.tokenizer.return_attention_mask)\n",
    "\n",
    "\n",
    "    def _process_one_sample(self, data, idx = None):\n",
    "\n",
    "        question = data[\"question\"]\n",
    "        table_column_names = data[\"table\"][\"header\"]\n",
    "        table_content_values = data[\"table\"][\"rows\"]\n",
    "\n",
    "        answer = data[\"answers\"]\n",
    "        answer_list = answers = [str(a).lower() for a in data[\"answers\"]]\n",
    "        answer = f\", \".join(answer).lower()\n",
    "\n",
    "        output_text = \"\"\n",
    "        input_text = f\"Question: {question} \"\n",
    "\n",
    "\n",
    "        if self.config.tokenizer.special_table_tok:\n",
    "\n",
    "            table = pd.DataFrame.from_dict({str(col).lower(): [str(table_content_values[j][i]).lower() for j in range(len(table_content_values))] for i, col in enumerate(table_column_names)})\n",
    "\n",
    "            if self.config.data.decompose_table:\n",
    "                relevant_rows, relevant_columns = self._decompose_table(question, answer_list, table)\n",
    "                \n",
    "                if self.config.training.training_type != \"table_decomposition\":\n",
    "                    \n",
    "                    if len(relevant_rows) > 0:\n",
    "                        table = table.iloc[relevant_rows]\n",
    "                    \n",
    "                    elif len(relevant_columns) > 0:\n",
    "                        table = table[relevant_columns]\n",
    "                else:\n",
    "                    if len(relevant_rows) > 0:\n",
    "                        table_output = table.iloc[relevant_rows]\n",
    "                    \n",
    "                    elif len(relevant_columns) > 0:\n",
    "                        table_output = table[relevant_columns]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            if self.config.data.decompose_table:\n",
    "                table = pd.DataFrame.from_dict({str(col).lower(): [str(table_content_values[j][i]).lower() for j in range(len(table_content_values))] for i, col in enumerate(table_column_names)})\n",
    "                relevant_rows, relevant_columns = self._decompose_table(question, answer_list, table)\n",
    "                \n",
    "                if self.config.training.training_type != \"table_decomposition\":\n",
    "                    if len(relevant_rows) > 0:\n",
    "                        table = table.iloc[relevant_rows]\n",
    "                    \n",
    "                    elif len(relevant_columns) > 0:\n",
    "                        table = table[relevant_columns]\n",
    "\n",
    "                    table_column_names = table.columns.tolist()\n",
    "                    table_content_values = table.values.tolist()\n",
    "\n",
    "                else:\n",
    "                    if len(relevant_rows) > 0:\n",
    "                        table_output = table.iloc[relevant_rows]\n",
    "                    \n",
    "                    elif len(relevant_columns) > 0:\n",
    "                        table_output = table[relevant_columns]\n",
    "\n",
    "\n",
    "            table = \"[HEADER] \" + \" | \".join(table_column_names)\n",
    "            for row_id, row in enumerate(table_content_values):\n",
    "                table += f\" [ROW] {row_id}: \" + \" | \".join(row) \n",
    "\n",
    "            if self.config.training.training_type == \"table_decomposition\":\n",
    "                table_column_names_output = table_output.columns.tolist()\n",
    "                table_content_values_output = table_output.values.tolist()\n",
    "\n",
    "                table_output = \"[HEADER] \" + \" | \".join(table_column_names_output)\n",
    "                for row_id, row in enumerate(table_content_values_output):\n",
    "                    table_output += f\" [ROW] {row_id}: \" + \" | \".join(row)\n",
    "\n",
    "        if self.config.training.training_type == \"table_decomposition\":\n",
    "            return question, table, table_output\n",
    "        else:\n",
    "            return input_text, table, output_text\n",
    "\n",
    "    \n",
    "\n",
    "    def _process_dataset(self):\n",
    "\n",
    "        processed_data = []\n",
    "        for i, data in tqdm(enumerate(self.dataset[self.data_type]), position = 0, leave = True, total = len(self.dataset[self.data_type])):\n",
    "            processed_data.append(self._process_one_sample(data, i))\n",
    "\n",
    "\n",
    "        text_input = [x[0] for x in processed_data]\n",
    "        table = [x[1] for x in processed_data]\n",
    "        text_output = [x[2] for x in processed_data]\n",
    "\n",
    "        return text_input, table, text_output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_input)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        tokenized_input = self._tokenize(self.text_input[index], self.table[index], max_length = self.config.tokenizer.input_max_length)\n",
    "        return tokenized_input[\"input_ids\"].squeeze(), tokenized_input[\"attention_mask\"].squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitablequestions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"configs/wiki_tq_reasoning/t5.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "config = process_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = WikiTQReasoningWithoutAnswerDataset(dataset = dataset, config = config, data_type = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size = 16, shuffle = False, num_workers = config.system.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_generations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ModelForTableReasoning(config)\n",
    "model.load_state_dict(torch.load(\"logs/table_question_reasoning_flan_t5_xl_reason_without_answer_new/checkpoints/epoch=50.pt\", map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in tqdm(enumerate(train_dataloader), position = 0, leave = True, total = len(train_dataloader)):\n",
    "\n",
    "    input_ids, attention_mask = batch\n",
    "    predicted_ids = model.model.generate(input_ids = input_ids.to(\"cuda:0\"), attention_mask = attention_mask.to(\"cuda:0\"), \n",
    "                                     max_new_tokens = config.tokenizer.output_max_length, num_beams = 3, early_stopping = True).detach().cpu()\n",
    "\n",
    "    batch_predicted_reason = train_dataset.tokenizer.batch_decode(predicted_ids, skip_special_tokens = True)\n",
    "\n",
    "    reason_generations.extend(batch_predicted_reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reason_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"datasets/test_wiki_tq_no_answer_in_reason_flant5.pkl\", \"wb\") as f:\n",
    "    pickle.dump(reason_generations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reason_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"datasets/wiki_tq_reason.pkl\", \"rb\") as f:\n",
    "    reason_generations_with_answer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total = 0\n",
    "for i in range(len(reason_generations)):\n",
    "    if reason_generations[i] != reason_generations_with_answer[i]:\n",
    "        count += 1\n",
    "    total += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "train_dataset = load_dataset(\"wikitablequestions\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = train_dataset[idx][\"question\"]\n",
    "answer = \", \".join(train_dataset[idx][\"answers\"]).lower()\n",
    "table_column_names = train_dataset[idx][\"table\"][\"header\"]\n",
    "table_content_values = train_dataset[idx][\"table\"][\"rows\"]\n",
    "\n",
    "table = pd.DataFrame.from_dict({str(col).lower(): [str(table_content_values[j][i]).lower() for j in range(len(table_content_values))] for i, col in enumerate(table_column_names)})\n",
    "\n",
    "reason = reason_generations[idx]\n",
    "reason_with_answer = reason_generations_with_answer[idx]\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "print(f\"reason by model trained without answer: {reason}\")\n",
    "print(f\"reason by model trained with answer: {reason_with_answer}\")\n",
    "\n",
    "display(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"datasets/test_wiki_tq_reason_without_answer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(reason_generations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Reasons on WikiTQ dataset using TAPEX trained with answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "from src import T5ModelForTableReasoning\n",
    "from utils import process_config\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiTQReasoningDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset, config):\n",
    "        super(WikiTQReasoningDataset, self).__init__()\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.config = config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer.tokenizer_path, local_files_only = self.config.tokenizer.local_files_only,\n",
    "                                                       padding_side = self.config.tokenizer.padding_side)\n",
    "        \n",
    "        if \"bos_token\" not in list(self.tokenizer.special_tokens_map.keys()):\n",
    "            self.tokenizer.add_special_tokens({\"bos_token\": \"<s>\"})\n",
    "\n",
    "        if \"pad_token\" not in list(self.tokenizer.special_tokens_map.keys()):\n",
    "            self.tokenizer.add_special_tokens({\"pad_token\": self.tokenizer.special_tokens_map[\"eos_token\"]})\n",
    "\n",
    "        if \"sep_token\" not in list(self.tokenizer.special_tokens_map.keys()):\n",
    "            self.tokenizer.add_special_tokens({\"sep_token\": self.tokenizer.special_tokens_map[\"eos_token\"]})\n",
    "\n",
    "        if \"mask_token\" not in list(self.tokenizer.special_tokens_map.keys()):\n",
    "            self.tokenizer.add_special_tokens({\"mask_token\": self.tokenizer.special_tokens_map[\"eos_token\"]})\n",
    "\n",
    "        \n",
    "        with open(\"datasets/wiki_tq_reason.pkl\", \"rb\") as f:\n",
    "            self.reasons = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "        self.text_input, self.table, self.text_output = self._process_dataset()\n",
    "        \n",
    "\n",
    "    def _tokenize(self, text_input, table = None, max_length = 512, text_output = None):\n",
    "\n",
    "        if text_output is not None:\n",
    "            if self.config.tokenizer.special_table_tok:\n",
    "                raise NotImplementedError\n",
    "            else:\n",
    "                if table is not None:\n",
    "                    table = table + f\" {self.tokenizer.special_tokens_map['sep_token']} \" + text_output\n",
    "                else:\n",
    "                    text_input = text_input + f\" {self.tokenizer.special_tokens_map['sep_token']} \" + text_output\n",
    "            # text_input = text_input + f\" {self.tokenizer.special_tokens_map['sep_token']} \" + text_output\n",
    "\n",
    "        if self.config.tokenizer.special_table_tok:\n",
    "            if table is not None:\n",
    "                return self.tokenizer(table, text_input, add_special_tokens = self.config.tokenizer.add_special_tokens,\n",
    "                            padding = self.config.tokenizer.padding, truncation = self.config.tokenizer.truncation, \n",
    "                            max_length = max_length, return_tensors = self.config.tokenizer.return_tensors,\n",
    "                            return_token_type_ids = self.config.tokenizer.return_token_type_ids,\n",
    "                            return_attention_mask = self.config.tokenizer.return_attention_mask)\n",
    "            else: \n",
    "                return self.tokenizer(answer = text_input, add_special_tokens = self.config.tokenizer.add_special_tokens,\n",
    "                            padding = self.config.tokenizer.padding, truncation = self.config.tokenizer.truncation, \n",
    "                            max_length = max_length, return_tensors = self.config.tokenizer.return_tensors,\n",
    "                            return_token_type_ids = self.config.tokenizer.return_token_type_ids,\n",
    "                            return_attention_mask = self.config.tokenizer.return_attention_mask)\n",
    "        else:\n",
    "            if table is not None:\n",
    "                return self.tokenizer(text_input, table, add_special_tokens = self.config.tokenizer.add_special_tokens,\n",
    "                            padding = self.config.tokenizer.padding, truncation = self.config.tokenizer.truncation, \n",
    "                            max_length = max_length, return_tensors = self.config.tokenizer.return_tensors,\n",
    "                            return_token_type_ids = self.config.tokenizer.return_token_type_ids,\n",
    "                            return_attention_mask = self.config.tokenizer.return_attention_mask)\n",
    "            else:\n",
    "                return self.tokenizer(text_input, add_special_tokens = self.config.tokenizer.add_special_tokens,\n",
    "                            padding = self.config.tokenizer.padding, truncation = self.config.tokenizer.truncation, \n",
    "                            max_length = max_length, return_tensors = self.config.tokenizer.return_tensors,\n",
    "                            return_token_type_ids = self.config.tokenizer.return_token_type_ids,\n",
    "                            return_attention_mask = self.config.tokenizer.return_attention_mask)\n",
    "\n",
    "\n",
    "    def _process_one_sample(self, data, idx = None):\n",
    "\n",
    "        question = data[\"question\"]\n",
    "        table_column_names = data[\"table\"][\"header\"]\n",
    "        table_content_values = data[\"table\"][\"rows\"]\n",
    "\n",
    "        answer = data[\"answers\"]\n",
    "        answer_list = answers = [str(a).lower() for a in data[\"answers\"]]\n",
    "        answer = f\", \".join(answer).lower()\n",
    "\n",
    "\n",
    "        # question = self.dataset[\"question\"][idx]\n",
    "        # table_dict = eval(self.dataset[\"table\"][idx])\n",
    "        # table_column_names = table_dict[\"header\"]\n",
    "        # table_content_values = table_dict[\"rows\"]\n",
    "\n",
    "        # answer = eval(self.dataset[\"answers\"][idx])\n",
    "        # answer_list = answers = [str(a).lower() for a in self.dataset[\"answers\"]]\n",
    "        # answer = f\", \".join(answer).lower()\n",
    "\n",
    "        output_text = self.reasons[idx]\n",
    "        input_text = f\"Question: {question} Answer: {answer}. \"\n",
    "\n",
    "\n",
    "        if self.config.tokenizer.special_table_tok:\n",
    "            \n",
    "            # table_content_values = [self.expand_numbers(table_content_values[i]) for i in range(len(table_content_values))]\n",
    "\n",
    "            # table_content_values = [[self.expand_numbers(table_content_values[i][j]) for j in range(len(table_content_values[i]))] for i in range(len(table_content_values))]\n",
    "\n",
    "            # for i in range(table_content_values):\n",
    "            #     for j in range(table_content_values[i]):\n",
    "            #         table_content_values[i][j] = self.expand_numbers(table_content_values[i][j])\n",
    "\n",
    "            table = pd.DataFrame.from_dict({str(col).lower(): [str(table_content_values[j][i]).lower() for j in range(len(table_content_values))] for i, col in enumerate(table_column_names)})\n",
    "\n",
    "            if self.config.data.decompose_table:\n",
    "                relevant_rows, relevant_columns = self._decompose_table(question, answer_list, table)\n",
    "                \n",
    "                if self.config.training.training_type != \"table_decomposition\":\n",
    "                    \n",
    "                    if len(relevant_rows) > 0:\n",
    "                        table = table.iloc[relevant_rows]\n",
    "                    \n",
    "                    elif len(relevant_columns) > 0:\n",
    "                        table = table[relevant_columns]\n",
    "                else:\n",
    "                    if len(relevant_rows) > 0:\n",
    "                        table_output = table.iloc[relevant_rows]\n",
    "                    \n",
    "                    elif len(relevant_columns) > 0:\n",
    "                        table_output = table[relevant_columns]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            if self.config.data.decompose_table:\n",
    "                table = pd.DataFrame.from_dict({str(col).lower(): [str(table_content_values[j][i]).lower() for j in range(len(table_content_values))] for i, col in enumerate(table_column_names)})\n",
    "                relevant_rows, relevant_columns = self._decompose_table(question, answer_list, table)\n",
    "                \n",
    "                if self.config.training.training_type != \"table_decomposition\":\n",
    "                    if len(relevant_rows) > 0:\n",
    "                        table = table.iloc[relevant_rows]\n",
    "                    \n",
    "                    elif len(relevant_columns) > 0:\n",
    "                        table = table[relevant_columns]\n",
    "\n",
    "                    table_column_names = table.columns.tolist()\n",
    "                    table_content_values = table.values.tolist()\n",
    "\n",
    "                else:\n",
    "                    if len(relevant_rows) > 0:\n",
    "                        table_output = table.iloc[relevant_rows]\n",
    "                    \n",
    "                    elif len(relevant_columns) > 0:\n",
    "                        table_output = table[relevant_columns]\n",
    "\n",
    "\n",
    "            table = \"[HEADER] \" + \" | \".join(table_column_names)\n",
    "            for row_id, row in enumerate(table_content_values):\n",
    "                table += f\" [ROW] {row_id}: \" + \" | \".join(row) \n",
    "\n",
    "            if self.config.training.training_type == \"table_decomposition\":\n",
    "                table_column_names_output = table_output.columns.tolist()\n",
    "                table_content_values_output = table_output.values.tolist()\n",
    "\n",
    "                table_output = \"[HEADER] \" + \" | \".join(table_column_names_output)\n",
    "                for row_id, row in enumerate(table_content_values_output):\n",
    "                    table_output += f\" [ROW] {row_id}: \" + \" | \".join(row)\n",
    "\n",
    "        if self.config.training.training_type == \"table_decomposition\":\n",
    "            return question, table, table_output\n",
    "        else:\n",
    "            return input_text, table, output_text\n",
    "\n",
    "    \n",
    "    def _process_dataset(self):\n",
    "\n",
    "        # processed_data = Parallel(n_jobs = 1)(\n",
    "        #     delayed(self._process_one_sample)(data, i) for i, data in tqdm(enumerate(self.dataset[self.data_type]), position = 0, leave = True, total = len(self.dataset[self.data_type])) if i < 1000\n",
    "        # )\n",
    "\n",
    "        processed_data = []\n",
    "        for i, data in tqdm(enumerate(self.dataset), position = 0, leave = True, total = len(self.dataset)):\n",
    "            processed_data.append(self._process_one_sample(data, i))\n",
    "\n",
    "\n",
    "        text_input = [x[0] for x in processed_data]\n",
    "        table = [x[1] for x in processed_data]\n",
    "        text_output = [x[2] for x in processed_data]\n",
    "\n",
    "        return text_input, table, text_output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_input)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        \n",
    "        # NOTE: Currently the implementation of row embeddings, column embeddings and segment embeddings is available for encode-decoder models\n",
    "\n",
    "        # NOTE: Permute the rows and columns randomly\n",
    "        # self.table[index] = self.table[index].sample(frac = 1, axis = 1)\n",
    "\n",
    "        if self.config.model.type == \"encoder-decoder\":\n",
    "            if self.config.model.use_table:\n",
    "                tokenized_input = self._tokenize(self.text_input[index], self.table[index], max_length = self.config.tokenizer.input_max_length)\n",
    "            else:\n",
    "                tokenized_input = self._tokenize(self.text_input[index], max_length = self.config.tokenizer.input_max_length)\n",
    "\n",
    "            if self.config.training.training_type == \"description_generation\" or self.config.training.training_type == \"column_reasoning\" \\\n",
    "                  or self.config.training.training_type == \"table_question_answering\" or self.config.training.training_type == \"table_decomposition\" \\\n",
    "                    or self.config.training.training_type == \"table_reasoning\":\n",
    "                tokenized_output = self._tokenize(self.text_output[index], max_length = self.config.tokenizer.output_max_length)\n",
    "                labels = tokenized_output[\"input_ids\"][0].clone()\n",
    "                \n",
    "                if labels[0] == self.tokenizer.convert_tokens_to_ids(self.tokenizer.special_tokens_map[\"bos_token\"]):\n",
    "                    labels[:-1] = labels[1:].clone()\n",
    "                else:\n",
    "                    tokenized_output[\"input_ids\"][0][1:] = tokenized_output[\"input_ids\"][0][:-1].clone()\n",
    "                    tokenized_output[\"input_ids\"][0][0] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.special_tokens_map[\"bos_token\"])\n",
    "\n",
    "                labels[labels == self.tokenizer.convert_tokens_to_ids(self.tokenizer.special_tokens_map[\"pad_token\"])] = -100\n",
    "                # labels[labels == self.tokenizer.convert_tokens_to_ids(self.tokenizer.special_tokens_map[\"sep_token\"])] = -100\n",
    "                # labels[labels == self.tokenizer.convert_tokens_to_ids(self.tokenizer.special_tokens_map[\"bos_token\"])] = -100\n",
    "                # labels[labels == self.tokenizer.convert_tokens_to_ids(self.tokenizer.special_tokens_map[\"eos_token\"])] = -100\n",
    "\n",
    "            elif self.config.training.training_type == \"masked_language_modelling\":\n",
    "\n",
    "                mask_labels, desc_idx = self._whole_word_mask(self.tokenized_text[index])\n",
    "                mask_labels = torch.nonzero(mask_labels, as_tuple = True)[0] + 2\n",
    "\n",
    "                # Select the elements from the original tensor based on the random indices\n",
    "                mask_labels = mask_labels[mask_labels < self.config.tokenizer.input_max_length]\n",
    "                if mask_labels.size()[0] >= self.config.data.masked_gen_length // 2:\n",
    "                    mask_labels = mask_labels[:self.config.data.masked_gen_length // 2]\n",
    "\n",
    "                tokenized_output['input_ids'] = torch.ones(1, self.config.data.masked_gen_length, dtype = torch.long) * self.tokenizer.convert_tokens_to_ids(self.tokenizer.special_tokens_map[\"sep_token\"])\n",
    "                tokenized_output[\"input_ids\"][0][1:2*mask_labels.size()[0]:2] = tokenized_input[\"input_ids\"][0][mask_labels]\n",
    "                tokenized_output[\"input_ids\"][0][2*mask_labels.size()[0] + 1:] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.special_tokens_map[\"pad_token\"])\n",
    "                tokenized_output[\"input_ids\"][0][0] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.special_tokens_map[\"bos_token\"])\n",
    "\n",
    "                tokenized_input[\"input_ids\"][0][mask_labels] = self.tokenizer.mask_token_id\n",
    "\n",
    "                labels = tokenized_output[\"input_ids\"][0].clone()\n",
    "                labels[:-1] = labels[1:].clone()\n",
    "                labels[labels == self.tokenizer.convert_tokens_to_ids(self.tokenizer.special_tokens_map[\"pad_token\"])] = -100\n",
    "                labels[labels == self.tokenizer.convert_tokens_to_ids(self.tokenizer.special_tokens_map[\"sep_token\"])] = -100\n",
    "\n",
    "            if self.config.tokenizer.use_row_col_ids:\n",
    "                tokenized_text = self.tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"].squeeze(0))\n",
    "                tokenized_input[\"row_ids\"] = self._get_row_ids(tokenized_text = tokenized_text)\n",
    "                tokenized_input[\"col_ids\"] = self._get_col_ids(tokenized_text = tokenized_text)\n",
    "\n",
    "\n",
    "        # Tokenizers of decoder only models do not add start token, add them explicitly\n",
    "        elif self.config.model.type == \"decoder-only\":\n",
    "            tokenized_output = {}\n",
    "            if self.config.training.training_type == \"description_generation\" or self.config.training.training_type == \"column_reasoning\" \\\n",
    "                  or self.config.training.training_type == \"table_question_answering\" or self.config.training.training_type == \"table_decomposition\" \\\n",
    "                    or self.config.training.training_type == \"table_reasoning\":\n",
    "                \n",
    "                if self.config.model.use_table:\n",
    "                    tokenized_input = self._tokenize(self.text_input[index], self.table[index], max_length = self.config.tokenizer.input_max_length, text_output = self.text_output[index])\n",
    "                    inference_tokenized_input = self._tokenize(self.text_input[index], self.table[index], max_length = self.config.tokenizer.input_max_length)\n",
    "                else:\n",
    "                    tokenized_input = self._tokenize(self.text_input[index], max_length = self.config.tokenizer.input_max_length, text_output = self.text_output[index])\n",
    "                    inference_tokenized_input = self._tokenize(self.text_input[index], max_length = self.config.tokenizer.input_max_length)\n",
    "\n",
    "                idx = (inference_tokenized_input[\"input_ids\"][0] == self.tokenizer.convert_tokens_to_ids(self.tokenizer.special_tokens_map[\"eos_token\"])).nonzero(as_tuple = True)[0]\n",
    "                if len(idx) != 0:\n",
    "                    idx = idx[0]\n",
    "                    inference_tokenized_input[\"input_ids\"][0] = inference_tokenized_input[\"input_ids\"][0]\n",
    "                    inference_tokenized_input[\"attention_mask\"][0][:idx] = 0\n",
    "                    inference_tokenized_input[\"attention_mask\"][0][idx:] = 1\n",
    "\n",
    "                padded_input = torch.ones(self.config.tokenizer.input_max_length, dtype = torch.long) * self.tokenizer.convert_tokens_to_ids(self.tokenizer.special_tokens_map[\"eos_token\"])\n",
    "                padded_input[self.config.tokenizer.input_max_length - inference_tokenized_input[\"input_ids\"][0].shape[0]:] = inference_tokenized_input[\"input_ids\"][0]\n",
    "                inference_tokenized_input[\"input_ids\"][0] = padded_input\n",
    "\n",
    "                labels = tokenized_input[\"input_ids\"][0].clone()\n",
    "                actual_output_ids = self._tokenize(self.text_output[index], max_length = self.config.tokenizer.output_max_length)[\"input_ids\"].squeeze()\n",
    "\n",
    "                indices = (labels == self.tokenizer.convert_tokens_to_ids(self.tokenizer.special_tokens_map[\"sep_token\"])).nonzero(as_tuple = True)[0]\n",
    "                if len(indices) >= 2:\n",
    "                    out_start, out_end = indices[0] + 1, indices[1]\n",
    "                    labels[:out_start], labels[out_end:] = -100, -100\n",
    "                elif len(indices) == 1:\n",
    "                    out_start = indices[0] + 1\n",
    "                    labels[:out_start] = -100\n",
    "                else:\n",
    "                    labels[:] = -100\n",
    "                    labels[0] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.special_tokens_map[\"eos_token\"])\n",
    "\n",
    "\n",
    "                tokenized_input[\"input_ids\"][0][1:] = tokenized_input[\"input_ids\"][0].clone()[:-1]\n",
    "                tokenized_input[\"input_ids\"][0][0] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.special_tokens_map[\"bos_token\"])\n",
    "\n",
    "            elif self.config.training.training_type == \"masked_language_modelling\":\n",
    "\n",
    "                if self.config.model.use_table:\n",
    "                    tokenized_input = self._tokenize(self.text_input[index], self.table[index], max_length = self.config.tokenizer.input_max_length)\n",
    "                else:\n",
    "                    tokenized_input = self._tokenize(self.text_input[index], max_length = self.config.tokenizer.input_max_length)\n",
    "\n",
    "                tokenized_input[\"input_ids\"][0][1:] = tokenized_input[\"input_ids\"][0].clone()[:-1]\n",
    "                tokenized_input[\"input_ids\"][0][0] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.special_tokens_map[\"bos_token\"])\n",
    "\n",
    "                mask_labels, desc_idx = self._whole_word_mask(self.tokenized_text[index])\n",
    "                mask_labels = torch.nonzero(mask_labels, as_tuple = True)[0] + 2\n",
    "\n",
    "                # Select the elements from the original tensor based on the random indices\n",
    "                mask_labels = mask_labels[mask_labels < self.config.tokenizer.input_max_length]\n",
    "                if mask_labels.size()[0] >= self.config.data.masked_gen_length // 2:\n",
    "                    mask_labels = mask_labels[:self.config.data.masked_gen_length // 2]\n",
    "\n",
    "                eos_indices = (tokenized_input[\"input_ids\"][0] == self.tokenizer.convert_tokens_to_ids(self.tokenizer.special_tokens_map[\"eos_token\"])).nonzero(as_tuple = True)[0]\n",
    "                if len(eos_indices) < 4:\n",
    "                    # NOTE: No masking possible for this                    \n",
    "                    labels = torch.ones(tokenized_input[\"input_ids\"].shape[1], dtype = torch.long) * (-100)\n",
    "                    labels[0] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.special_tokens_map[\"eos_token\"])\n",
    "                else:\n",
    "\n",
    "                    out_start = eos_indices[3]\n",
    "                    mask_labels = mask_labels[:(self.config.tokenizer.input_max_length - out_start) // 2]\n",
    "\n",
    "                    labels = torch.ones(tokenized_input[\"input_ids\"].shape[1], dtype = torch.long) * (-100)\n",
    "                    labels[out_start:out_start + 2*mask_labels.size()[0]:2] = tokenized_input[\"input_ids\"][0][mask_labels]\n",
    "                    labels[:-1] = labels[1:].clone()\n",
    "\n",
    "                    tokenized_input[\"input_ids\"][0][mask_labels] = self.tokenizer.mask_token_id\n",
    "\n",
    "                    labels[labels == self.tokenizer.convert_tokens_to_ids(self.tokenizer.special_tokens_map[\"pad_token\"])] = -100\n",
    "                    labels[labels == self.tokenizer.convert_tokens_to_ids(self.tokenizer.special_tokens_map[\"sep_token\"])] = -100\n",
    "                    labels[labels == self.tokenizer.convert_tokens_to_ids(self.tokenizer.special_tokens_map[\"bos_token\"])] = -100\n",
    "                    labels[labels == self.tokenizer.convert_tokens_to_ids(self.tokenizer.special_tokens_map[\"eos_token\"])] = -100\n",
    "\n",
    "                    # NOTE: Discuss whether this is correct\n",
    "                    labels[0] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.special_tokens_map[\"eos_token\"])\n",
    "\n",
    "        # NOTE: Row and column ids is implemented only for encoder-decoder models\n",
    "        if self.config.model.type == \"encoder-decoder\":\n",
    "            if self.config.tokenizer.use_row_col_ids:\n",
    "                position_ids = torch.tensor([i for i in range(tokenized_input[\"input_ids\"].shape[1])], dtype = torch.long)\n",
    "                return tokenized_input[\"input_ids\"].squeeze(), tokenized_input[\"attention_mask\"].squeeze(), \\\n",
    "                        tokenized_input[\"token_type_ids\"].squeeze(), tokenized_output[\"input_ids\"].squeeze(), tokenized_input[\"row_ids\"].squeeze(), tokenized_input[\"col_ids\"].squeeze(), labels\n",
    "\n",
    "            else:\n",
    "                return tokenized_input[\"input_ids\"].squeeze(), tokenized_input[\"attention_mask\"].squeeze(), \\\n",
    "                        tokenized_input[\"token_type_ids\"].squeeze(), tokenized_output[\"input_ids\"].squeeze(), labels\n",
    "        \n",
    "        elif self.config.model.type == \"decoder-only\":\n",
    "\n",
    "            if self.config.training.training_type == \"description_generation\" or self.config.training.training_type == \"column_reasoning\" \\\n",
    "                  or self.config.training.training_type == \"table_question_answering\" or self.config.training.training_type == \"table_decomposition\" \\\n",
    "                    or self.config.training.training_type == \"table_reasoning\":\n",
    "                if self.config.model.use_position_ids:\n",
    "                    position_ids = torch.tensor([i for i in range(tokenized_input[\"input_ids\"].shape[1])], dtype = torch.long)\n",
    "                    return tokenized_input[\"input_ids\"].squeeze(), tokenized_input[\"attention_mask\"].squeeze(), \\\n",
    "                            tokenized_input[\"token_type_ids\"].squeeze(), position_ids, inference_tokenized_input[\"input_ids\"].squeeze(), inference_tokenized_input[\"attention_mask\"].squeeze(), actual_output_ids, labels\n",
    "\n",
    "                else:\n",
    "                    return tokenized_input[\"input_ids\"].squeeze(), tokenized_input[\"attention_mask\"].squeeze(), \\\n",
    "                            tokenized_input[\"token_type_ids\"].squeeze(), inference_tokenized_input[\"input_ids\"].squeeze(), inference_tokenized_input[\"attention_mask\"].squeeze(), actual_output_ids, labels\n",
    "            \n",
    "            else:\n",
    "                if self.config.model.use_position_ids:\n",
    "                    position_ids = torch.tensor([i for i in range(tokenized_input[\"input_ids\"].shape[1])], dtype = torch.long)\n",
    "                    return tokenized_input[\"input_ids\"].squeeze(), tokenized_input[\"attention_mask\"].squeeze(), \\\n",
    "                            tokenized_input[\"token_type_ids\"].squeeze(), position_ids, labels\n",
    "\n",
    "                else:\n",
    "                    return tokenized_input[\"input_ids\"].squeeze(), tokenized_input[\"attention_mask\"].squeeze(), \\\n",
    "                            tokenized_input[\"token_type_ids\"].squeeze(), labels\n",
    "\n",
    "    def collate_fn(self, items):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitablequestions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"configs/wiki_tq_reasoning/tapex.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "config = process_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = WikiTQReasoningDataset(dataset = dataset[\"train\"], config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size = 8, shuffle = False, num_workers = config.system.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import BartModelForTableReasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BartModelForTableReasoning(config)\n",
    "model.load_state_dict(torch.load(\"logs/table_question_reasoning_tapex_bootstrapping_baseline_loss_calc_change/checkpoints/epoch=30.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_generations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_reasons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in tqdm(enumerate(train_dataloader), position = 0, leave = True, total = len(train_dataloader)):\n",
    "\n",
    "    input_ids, attention_mask, _, _, labels = batch\n",
    "    predicted_ids = model.model.generate(input_ids = input_ids.to(\"cuda:0\"), attention_mask = attention_mask.to(\"cuda:0\"), \n",
    "                                     max_new_tokens = config.tokenizer.output_max_length, num_beams = 3, early_stopping = True).detach().cpu()\n",
    "\n",
    "    batch_predicted_reason = train_dataset.tokenizer.batch_decode(predicted_ids, skip_special_tokens = True)\n",
    "\n",
    "    reason_generations.extend(batch_predicted_reason)\n",
    "    actual_reasons.extend(train_dataset.tokenizer.batch_decode(labels[labels != -100], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reason_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset.text_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_generations = reason_generations[8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (reason, actual_reason) in enumerate(zip(reason_generations, train_dataset.text_output)):\n",
    "\n",
    "    if i > 3000 and i < 3010:\n",
    "        print(\"Generated Reason: \", reason)\n",
    "        print(\"Actual Reason: \", actual_reason)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove answer from reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/WikiTQReasoningData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasons_without_answer = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    reason = df[\"reason\"][i]\n",
    "    if \"from table\" in reason:\n",
    "        reason = reason.split(\"from table\")\n",
    "        if reason[0] == \"\":\n",
    "            reason = reason[1]\n",
    "        else:\n",
    "            reason = reason[0]\n",
    "\n",
    "    elif \"from the table\" in reason:\n",
    "        reason = reason.split(\"from the table\")[0]\n",
    "        if reason[0] == \"\":\n",
    "            reason = reason[1]\n",
    "        else:\n",
    "            reason = reason[0]\n",
    "\n",
    "    reasons_without_answer.append(reason.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = deepcopy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[\"reason\"] = reasons_without_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(\"datasets/WikiTQReasoningDataWithoutAnswer.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "new_df = pd.read_csv(\"datasets/WikiTQReasoningDataWithoutAnswer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import WikiTQReasoningWithoutAnswerDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from utils import process_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"configs/wiki_tq_reasoning/t5.json\", \"rb\") as f:\n",
    "    config = json.load(f)\n",
    "config = process_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(config.data.data_path)\n",
    "train_dataset = WikiTQReasoningWithoutAnswerDataset(dataset, config)\n",
    "tokenizer = train_dataset.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.text_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for i, text in enumerate(train_dataset.text_output):\n",
    "    if not isinstance(text, str):\n",
    "        print(i, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/WikiTQReasoningData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"reason\"][46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate reason on SequentialQA using Flan T5-xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import T5ModelForTableReasoning\n",
    "from data import SequentialQADataset\n",
    "from utils import process_config\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"configs/wiki_tq_reasoning/t5.json\", \"rb\") as f:\n",
    "    config = json.load(f)\n",
    "config = process_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.training.training_type = \"table_question_answering\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"msr_sqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SequentialQADataset(dataset = dataset, config = config, data_type = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ModelForTableReasoning(config)\n",
    "model.load_state_dict(torch.load(\"logs/table_question_reasoning_flan_t5_xl_reason_with_answer_rerun/checkpoints/epoch=50.pt\", map_location = \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size = 16, shuffle = False, num_workers = config.system.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_generations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in tqdm(enumerate(train_dataloader), position = 0, leave = True, total = len(train_dataloader)):\n",
    "\n",
    "    input_ids, attention_mask, _, _, labels = batch\n",
    "    predicted_ids = model.model.generate(input_ids = input_ids.to(\"cuda:0\"), attention_mask = attention_mask.to(\"cuda:0\"), \n",
    "                                     max_new_tokens = config.tokenizer.output_max_length, num_beams = 3, early_stopping = True).detach().cpu()\n",
    "\n",
    "    batch_predicted_reason = train_dataset.tokenizer.batch_decode(predicted_ids, skip_special_tokens = True)\n",
    "\n",
    "    reason_generations.extend(batch_predicted_reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"datasets/test_seq_qa_reason_without_answer_flant5.pkl\", \"wb\") as f:\n",
    "    pickle.dump(reason_generations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_generations[2229]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reason_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = train_dataset.text_input[13]\n",
    "table = train_dataset.table[13]\n",
    "reason = reason_generations[13]\n",
    "\n",
    "print(question, end = \"\\n\\n\")\n",
    "print(reason, end = \"\\n\\n\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reason generation on FetaQA using Flan T5 xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import T5ModelForTableReasoning\n",
    "from data import FetaQADataset\n",
    "from utils import process_config\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"configs/wiki_tq_reasoning/t5.json\", \"rb\") as f:\n",
    "    config = json.load(f)\n",
    "config = process_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.training.training_type = \"descriptive_table_question_answering\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"DongfuTingle/FeTaQA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FetaQADataset(dataset = dataset, config = config, data_type = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ModelForTableReasoning(config)\n",
    "model.load_state_dict(torch.load(\"/datadrive/tabllm/logs/table_question_reasoning_flan_t5_xl_reason_with_answer_rerun/checkpoints/epoch=50.pt\", map_location = \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size = 16, shuffle = False, num_workers = config.system.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_generations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in tqdm(enumerate(train_dataloader), position = 0, leave = True, total = len(train_dataloader)):\n",
    "\n",
    "    input_ids, attention_mask, _, _, labels = batch\n",
    "    predicted_ids = model.model.generate(input_ids = input_ids.to(\"cuda:0\"), attention_mask = attention_mask.to(\"cuda:0\"), \n",
    "                                     max_new_tokens = config.tokenizer.output_max_length, num_beams = 3, early_stopping = True).detach().cpu()\n",
    "\n",
    "    batch_predicted_reason = train_dataset.tokenizer.batch_decode(predicted_ids, skip_special_tokens = True)\n",
    "\n",
    "    reason_generations.extend(batch_predicted_reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"datasets/test_feta_qa_reason_without_answer_flant5.pkl\", \"wb\") as f:\n",
    "    pickle.dump(reason_generations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reason_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = train_dataset.text_input[1000]\n",
    "table = train_dataset.table[1000]\n",
    "answer = train_dataset.text_output[1000]\n",
    "reason = reason_generations[1000]\n",
    "\n",
    "print(text_input, end = \"\\n\\n\")\n",
    "print(answer, end = \"\\n\\n\")\n",
    "print(reason, end = \"\\n\\n\")\n",
    "print(table, end = \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
