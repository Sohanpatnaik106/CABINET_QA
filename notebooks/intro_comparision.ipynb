{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from data import WikiTQHighlightedCellsDataset\n",
    "import json\n",
    "from utils import process_config\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from src import HighlightedCluBartModelForGenerativeQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from codecs import open\n",
    "from math import isnan, isinf\n",
    "from easydict import EasyDict\n",
    "from torch.utils.data import DataLoader\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import collections\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "\n",
    "    if not isinstance(x, str):\n",
    "        x = x.decode('utf8', errors='ignore')\n",
    "\n",
    "    # Remove diacritics\n",
    "    x = ''.join(c for c in unicodedata.normalize('NFKD', x)\n",
    "                if unicodedata.category(c) != 'Mn')\n",
    "    \n",
    "    # Normalize quotes and dashes\n",
    "    x = re.sub(r\"[‘’´`]\", \"'\", x)\n",
    "    x = re.sub(r\"[“”]\", \"\\\"\", x)\n",
    "    x = re.sub(r\"[‐‑‒–—−]\", \"-\", x)\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        old_x = x\n",
    "\n",
    "        # Remove citations\n",
    "        x = re.sub(r\"((?<!^)\\[[^\\]]*\\]|\\[\\d+\\]|[•♦†‡*#+])*$\", \"\", x.strip())\n",
    "        \n",
    "        # Remove details in parenthesis\n",
    "        x = re.sub(r\"(?<!^)( \\([^)]*\\))*$\", \"\", x.strip())\n",
    "        \n",
    "        # Remove outermost quotation mark\n",
    "        x = re.sub(r'^\"([^\"]*)\"$', r'\\1', x.strip())\n",
    "        \n",
    "        if x == old_x:\n",
    "            break\n",
    "    \n",
    "    # Remove final '.'\n",
    "    if x and x[-1] == '.':\n",
    "        x = x[:-1]\n",
    "    \n",
    "    # Collapse whitespaces and convert to lower case\n",
    "    x = re.sub(r'\\s+', ' ', x, flags=re.U).lower().strip()\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "class Value(object):\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    # Should be populated with the normalized string\n",
    "    _normalized = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def match(self, other):\n",
    "        \"\"\"Return True if the value matches the other value.\n",
    "\n",
    "        Args:\n",
    "            other (Value)\n",
    "        Returns:\n",
    "            a boolean\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def normalized(self):\n",
    "        return self._normalized\n",
    "\n",
    "\n",
    "class StringValue(Value):\n",
    "\n",
    "    def __init__(self, content):\n",
    "        assert isinstance(content, str)\n",
    "        self._normalized = normalize(content)\n",
    "        self._hash = hash(self._normalized)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, StringValue) and self.normalized == other.normalized\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self._hash\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'S' + str([self.normalized])\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "    def match(self, other):\n",
    "        assert isinstance(other, Value)\n",
    "        return self.normalized == other.normalized\n",
    "\n",
    "\n",
    "class NumberValue(Value):\n",
    "\n",
    "    def __init__(self, amount, original_string=None):\n",
    "        assert isinstance(amount, (int, float))\n",
    "        if abs(amount - round(amount)) < 1e-6:\n",
    "            self._amount = int(amount)\n",
    "        else:\n",
    "            self._amount = float(amount)\n",
    "        if not original_string:\n",
    "            self._normalized = str(self._amount)\n",
    "        else:\n",
    "            self._normalized = normalize(original_string)\n",
    "        self._hash = hash(self._amount)\n",
    "\n",
    "    @property\n",
    "    def amount(self):\n",
    "        return self._amount\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, NumberValue) and self.amount == other.amount\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self._hash\n",
    "\n",
    "    def __str__(self):\n",
    "        return ('N(%f)' % self.amount) + str([self.normalized])\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "    def match(self, other):\n",
    "        assert isinstance(other, Value)\n",
    "        if self.normalized == other.normalized:\n",
    "            return True\n",
    "        if isinstance(other, NumberValue):\n",
    "            return abs(self.amount - other.amount) < 1e-6\n",
    "        return False\n",
    "\n",
    "    @staticmethod\n",
    "    def parse(text):\n",
    "        \"\"\"Try to parse into a number.\n",
    "\n",
    "        Return:\n",
    "            the number (int or float) if successful; otherwise None.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return int(text)\n",
    "        except:\n",
    "            try:\n",
    "                amount = float(text)\n",
    "                assert not isnan(amount) and not isinf(amount)\n",
    "                return amount\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "\n",
    "class DateValue(Value):\n",
    "\n",
    "    def __init__(self, year, month, day, original_string=None):\n",
    "\n",
    "        \"\"\"Create a new DateValue. Placeholders are marked as -1.\"\"\"\n",
    "        assert isinstance(year, int)\n",
    "        assert isinstance(month, int) and (month == -1 or 1 <= month <= 12)\n",
    "        assert isinstance(day, int) and (day == -1 or 1 <= day <= 31)\n",
    "        assert not (year == month == day == -1)\n",
    "        \n",
    "        self._year = year\n",
    "        self._month = month\n",
    "        self._day = day\n",
    "        \n",
    "        if not original_string:\n",
    "            self._normalized = '{}-{}-{}'.format(\n",
    "                year if year != -1 else 'xx',\n",
    "                month if month != -1 else 'xx',\n",
    "                day if day != '-1' else 'xx')\n",
    "        else:\n",
    "            self._normalized = normalize(original_string)\n",
    "        \n",
    "        self._hash = hash((self._year, self._month, self._day))\n",
    "\n",
    "    @property\n",
    "    def ymd(self):\n",
    "        return (self._year, self._month, self._day)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, DateValue) and self.ymd == other.ymd\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self._hash\n",
    "\n",
    "    def __str__(self):\n",
    "        return (('D(%d,%d,%d)' % (self._year, self._month, self._day))\n",
    "                + str([self._normalized]))\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "    def match(self, other):\n",
    "        \n",
    "        assert isinstance(other, Value)\n",
    "        \n",
    "        if self.normalized == other.normalized:\n",
    "            return True\n",
    "        \n",
    "        if isinstance(other, DateValue):\n",
    "            return self.ymd == other.ymd\n",
    "        \n",
    "        return False\n",
    "\n",
    "    @staticmethod\n",
    "    def parse(text):\n",
    "        \"\"\"Try to parse into a date.\n",
    "\n",
    "        Return:\n",
    "            tuple (year, month, date) if successful; otherwise None.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            ymd = text.lower().split('-')\n",
    "            assert len(ymd) == 3\n",
    "            year = -1 if ymd[0] in ('xx', 'xxxx') else int(ymd[0])\n",
    "            month = -1 if ymd[1] == 'xx' else int(ymd[1])\n",
    "            day = -1 if ymd[2] == 'xx' else int(ymd[2])\n",
    "            assert not (year == month == day == -1)\n",
    "            assert month == -1 or 1 <= month <= 12\n",
    "            assert day == -1 or 1 <= day <= 31\n",
    "            return (year, month, day)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "def to_value(original_string, corenlp_value=None):\n",
    "    \"\"\"Convert the string to Value object.\n",
    "\n",
    "    Args:\n",
    "        original_string (basestring): Original string\n",
    "        corenlp_value (basestring): Optional value returned from CoreNLP\n",
    "    Returns:\n",
    "        Value\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(original_string, Value):\n",
    "        # Already a Value\n",
    "        return original_string\n",
    "    \n",
    "    if not corenlp_value:\n",
    "        corenlp_value = original_string\n",
    "    \n",
    "    # Number?\n",
    "    amount = NumberValue.parse(corenlp_value)\n",
    "    \n",
    "    if amount is not None:\n",
    "        return NumberValue(amount, original_string)\n",
    "    \n",
    "    # Date?\n",
    "    ymd = DateValue.parse(corenlp_value)\n",
    "    if ymd is not None:\n",
    "        if ymd[1] == ymd[2] == -1:\n",
    "            return NumberValue(ymd[0], original_string)\n",
    "        else:\n",
    "            return DateValue(ymd[0], ymd[1], ymd[2], original_string)\n",
    "    \n",
    "    # String.\n",
    "    return StringValue(original_string)\n",
    "\n",
    "\n",
    "def to_value_list(original_strings, corenlp_values=None):\n",
    "    \"\"\"Convert a list of strings to a list of Values\n",
    "\n",
    "    Args:\n",
    "        original_strings (list[basestring])\n",
    "        corenlp_values (list[basestring or None])\n",
    "    Returns:\n",
    "        list[Value]\n",
    "    \"\"\"\n",
    "    assert isinstance(original_strings, (list, tuple, set))\n",
    "    if corenlp_values is not None:\n",
    "        assert isinstance(corenlp_values, (list, tuple, set))\n",
    "        assert len(original_strings) == len(corenlp_values)\n",
    "        return list(set(to_value(x, y) for (x, y)\n",
    "                        in zip(original_strings, corenlp_values)))\n",
    "    else:\n",
    "        return list(set(to_value(x) for x in original_strings))\n",
    "\n",
    "\n",
    "def check_denotation(target_values, predicted_values):\n",
    "    \"\"\"Return True if the predicted denotation is correct.\n",
    "\n",
    "    Args:\n",
    "        target_values (list[Value])\n",
    "        predicted_values (list[Value])\n",
    "    Returns:\n",
    "        bool\n",
    "    \"\"\"\n",
    "\n",
    "    # Check size\n",
    "    if len(target_values) != len(predicted_values):\n",
    "        return False\n",
    "    \n",
    "    # Check items\n",
    "    for target in target_values:\n",
    "        if not any(target.match(pred) for pred in predicted_values):\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def tsv_unescape(x):\n",
    "    \"\"\"Unescape strings in the TSV file.\n",
    "    Escaped characters include:\n",
    "        newline (0x10) -> backslash + n\n",
    "        vertical bar (0x7C) -> backslash + p\n",
    "        backslash (0x5C) -> backslash + backslash\n",
    "\n",
    "    Args:\n",
    "        x (str or unicode)\n",
    "    Returns:\n",
    "        a unicode\n",
    "    \"\"\"\n",
    "    return x.replace(r'\\n', '\\n').replace(r'\\p', '|').replace('\\\\\\\\', '\\\\')\n",
    "\n",
    "\n",
    "def tsv_unescape_list(x):\n",
    "    \"\"\"Unescape a list in the TSV file.\n",
    "    List items are joined with vertical bars (0x5C)\n",
    "\n",
    "    Args:\n",
    "        x (str or unicode)\n",
    "    Returns:\n",
    "        a list of unicodes\n",
    "    \"\"\"\n",
    "    return [tsv_unescape(y) for y in x.split('|')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dater_outputs/gloc_wtq_end2end_wikitq_test.json','r') as f:\n",
    "    dic = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dater_error_indices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dic:\n",
    "\n",
    "    id = key\n",
    "    to_union = collections.defaultdict(float)\n",
    "    it = dic[key]\n",
    "    table = it['data_item']['table_text']\n",
    "    st = it['data_item']['statement']\n",
    "    ######### col filed################\n",
    "    preds = it['generations']\n",
    "    actual_answer = it['data_item']['answer']\n",
    "    \n",
    "    for pred in preds:\n",
    "        log_prob_mean = pred[2]\n",
    "        pred = pred[0]\n",
    "\n",
    "        pred = pred.split('therefore,the answer is :')[-1]\n",
    "        \n",
    "        key = pred\n",
    "        to_union[key] += np.exp(log_prob_mean)\n",
    "    d_ordered = sorted(to_union.items(),key=lambda x:x[1],reverse=True)\n",
    "    try:\n",
    "        pred_answer = d_ordered[0][0].split('\\n')[0].strip()\n",
    "    except Exception:\n",
    "        pred_answer = 'error'\n",
    "    st = st.split('\\n')[0]\n",
    "    # treat pred as an single entity because of 2 prompt with out example\n",
    "    # if len(target_values) == 1:\n",
    "    #     pred_answer = [pred_answer]\n",
    "    # else:\n",
    "    #     pred_answer = pred_answer.split(',')\n",
    "    \n",
    "    # treat pred answer as only on entity\n",
    "    pred_answer = [pred_answer]\n",
    "    pred_answer = to_value_list(pred_answer)\n",
    "\n",
    "    actual_answer = to_value_list(actual_answer)\n",
    "\n",
    "    correct = check_denotation(actual_answer, pred_answer)\n",
    "\n",
    "    if not correct:\n",
    "        dater_error_indices.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"configs/wiki_tq/tapex.json\", \"rb\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "config = process_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(config.data.data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = WikiTQHighlightedCellsDataset(dataset=dataset, config=config, data_type=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = test_dataset.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HighlightedCluBartModelForGenerativeQuestionAnswering(config=config)\n",
    "model.load_state_dict(torch.load(\"omnitab_best_ckpt/epoch=28.pt\", map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(index):\n",
    "\n",
    "    input_ids, attention_mask, token_type_ids, decoder_input_ids, highlighted_cells, labels = test_dataset.__getitem__(index)\n",
    "    input_ids = input_ids.unsqueeze(0).to(\"cuda:0\")\n",
    "    attention_mask = attention_mask.unsqueeze(0).to(\"cuda:0\")\n",
    "    decoder_input_ids = decoder_input_ids.unsqueeze(0).to(\"cuda:0\")\n",
    "    highlighted_cells = highlighted_cells.unsqueeze(0).to(\"cuda:0\")\n",
    "    labels = labels.unsqueeze(0).to(\"cuda:0\")\n",
    "\n",
    "    inputs_embeds = model.model.model.decomposer.embed_tokens(input_ids) * model.model.model.decomposer.embed_scale\n",
    "\n",
    "    decomposer_outputs = model.model.model.decomposer(input_ids=None,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None,\n",
    "        )\n",
    "    \n",
    "    latent_rep = model.model.model.latent_rep_head(decomposer_outputs[0])\n",
    "    # cluster_labels = torch.norm(latent_rep.unsqueeze(2) - model.model.model.cluster_centers.unsqueeze(0).unsqueeze(0), dim = -1).squeeze().argmin(dim = -1)\n",
    "\n",
    "    soft_labels_numerator = (1 + torch.norm((latent_rep.unsqueeze(2) - model.model.model.cluster_centers.unsqueeze(0).unsqueeze(0)), dim = -1) / model.model.model.clu_alpha) ** (-(1 + model.model.model.clu_alpha) / 2)\n",
    "    soft_labels = soft_labels_numerator / torch.sum(soft_labels_numerator, dim = -1).unsqueeze(-1)\n",
    "\n",
    "    token_scores_1 = model.model.model.token_classifier_score1(latent_rep)\n",
    "    token_scores_2 = model.model.model.token_classifier_score2(latent_rep)\n",
    "    gaussian_rvs = model.model.gaussian_dist.sample(token_scores_1.shape).to(token_scores_1.device)\n",
    "    relevance_logit = gaussian_rvs * token_scores_1 + token_scores_2\n",
    "    relevance_score = model.model.model.sigmoid(relevance_logit)\n",
    "\n",
    "    # NOTE: Uncomment as per requirement of the experiment\n",
    "    \n",
    "    relevance_score = (0.7 * relevance_score + 0.3 * highlighted_cells.unsqueeze(-1)).squeeze()\n",
    "\n",
    "    print(test_dataset.tokenizer.decode(input_ids[0], skip_special_tokens = True))\n",
    "    print(test_dataset.tokenizer.decode(input_ids[0][relevance_score >= relevance_score.mean()]))\n",
    "    print(test_dataset.tokenizer.decode(labels[labels != -100]))\n",
    "\n",
    "    output_ids = model.model.generate(input_ids = input_ids.to(\"cuda:0\"), max_new_tokens = config.tokenizer.output_max_length, \n",
    "                            num_beams = 3, early_stopping = True, attention_mask = attention_mask.to(\"cuda:0\"), \n",
    "                            highlighted_cells = highlighted_cells.to(\"cuda:0\")).squeeze()\n",
    "    \n",
    "    print(tokenizer.decode(output_ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, id in enumerate(dater_error_indices):\n",
    "\n",
    "\n",
    "    if i < 47:\n",
    "        continue\n",
    "\n",
    "    print(id)\n",
    "    predict(int(id))\n",
    "    \n",
    "    to_union = collections.defaultdict(float)\n",
    "    it = dic[id]\n",
    "    table = it['data_item']['table_text']\n",
    "    st = it['data_item']['statement']\n",
    "    ######### col filed################\n",
    "    preds = it['generations']\n",
    "    actual_answer = it['data_item']['answer']\n",
    "    \n",
    "    for pred in preds:\n",
    "        log_prob_mean = pred[2]\n",
    "        pred = pred[0]\n",
    "\n",
    "        pred = pred.split('therefore,the answer is :')[-1]\n",
    "        \n",
    "        key = pred\n",
    "        to_union[key] += np.exp(log_prob_mean)\n",
    "    d_ordered = sorted(to_union.items(),key=lambda x:x[1],reverse=True)\n",
    "    try:\n",
    "        pred_answer = d_ordered[0][0].split('\\n')[0].strip()\n",
    "    except Exception:\n",
    "        pred_answer = 'error'\n",
    "\n",
    "    print(\"DATER prediction: \", pred_answer)\n",
    "    print(\"DATER statement: \", st)\n",
    "\n",
    "    print(\"DATER table\", table)\n",
    "    table_column_names = table[0]\n",
    "    table_content_values = table[1:]\n",
    "    \n",
    "    table_df = pd.DataFrame.from_dict({str(col).lower(): [str(table_content_values[j][i]).lower() for j in range(len(table_content_values))] for i, col in enumerate(table_column_names)})\n",
    "\n",
    "    display(table_df)\n",
    "\n",
    "\n",
    "    print(\"Actual table\")\n",
    "\n",
    "    table_column_names = dataset[\"test\"][int(id)][\"table\"][\"header\"]\n",
    "    table_content_values = dataset[\"test\"][int(id)][\"table\"][\"rows\"]\n",
    "    actual_table_df = pd.DataFrame.from_dict({str(col).lower(): [str(table_content_values[j][i]).lower() for j in range(len(table_content_values))] for i, col in enumerate(table_column_names)})\n",
    "\n",
    "    display(actual_table_df)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "designlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
