{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from src import T5ModelForTableReasoning, T5ModelForTableCellHighlighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import WikiTQHighlightedCellsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from utils import process_config\n",
    "with open(\"configs/wiki_tq_clustering_and_highlighting/tapex.json\", \"rb\") as f:\n",
    "    config = json.load(f)\n",
    "config = process_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitablequestions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = WikiTQHighlightedCellsDataset(dataset = dataset, config = config, data_type = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = test_dataset.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_tables = test_dataset.table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "original_tables_num_column = np.array([len(x.columns.values) for x in original_tables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_tables_num_column[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_tables = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_table(table, text_input, idx = None):\n",
    "\n",
    "    tokenized_len = tokenizer(table = table, text_input = text_input, add_special_tokens = config.tokenizer.add_special_tokens, \n",
    "                 return_tensors = config.tokenizer.return_tensors)[\"input_ids\"][0].shape[0]\n",
    "    \n",
    "    if tokenized_len > 1000:\n",
    "        return table\n",
    "    \n",
    "    else:\n",
    "        num_columns = len(table.columns.values)\n",
    "        indices = np.where(original_tables_num_column == num_columns)[0]\n",
    "        \n",
    "        if len(indices) == 1:\n",
    "            return table\n",
    "        else:\n",
    "            indices = indices[indices != idx]\n",
    "            table_index = np.random.choice(indices)\n",
    "            num_rows = len(original_tables[table_index])\n",
    "\n",
    "            if tokenized_len > 900:\n",
    "                if num_rows < 1:\n",
    "                    return table\n",
    "                else:\n",
    "                    random_index = np.random.randint(0, num_rows)\n",
    "                    insert_index = np.random.randint(0, len(table))\n",
    "                    \n",
    "                    new_row = pd.DataFrame(original_tables[table_index].iloc[random_index]).T\n",
    "                    new_row.columns = table.columns.values\n",
    "                    perturbed_table = pd.concat([table.loc[:insert_index], new_row, table.loc[insert_index:]]).reset_index(drop=True)\n",
    "\n",
    "            elif tokenized_len > 800:\n",
    "                if num_rows < 2:\n",
    "                    return table\n",
    "                else:\n",
    "                    random_index = np.random.randint(0, num_rows, size = 2)\n",
    "                    insert_index = np.random.randint(0, len(table))\n",
    "                    \n",
    "                    new_row = pd.DataFrame(original_tables[table_index].iloc[random_index])\n",
    "                    new_row.columns = table.columns.values\n",
    "                    perturbed_table = pd.concat([table.loc[:insert_index], new_row, table.loc[insert_index:]]).reset_index(drop=True)\n",
    "            elif tokenized_len > 500:\n",
    "                if num_rows < 3:\n",
    "                    return table\n",
    "                else:\n",
    "                    random_index = np.random.randint(0, num_rows, size = 3)\n",
    "                    insert_index = np.random.randint(0, len(table))\n",
    "                    \n",
    "                    new_row = pd.DataFrame(original_tables[table_index].iloc[random_index])\n",
    "                    new_row.columns = table.columns.values\n",
    "                    perturbed_table = pd.concat([table.loc[:insert_index], new_row, table.loc[insert_index:]]).reset_index(drop=True)\n",
    "\n",
    "            else:\n",
    "                if num_rows < 5:\n",
    "                    return table\n",
    "                else:\n",
    "                    random_index = np.random.randint(0, num_rows, size = 5)\n",
    "                    insert_index = np.random.randint(0, len(table))                    \n",
    "                    \n",
    "                    new_row = pd.DataFrame(original_tables[table_index].iloc[random_index])\n",
    "                    new_row.columns = table.columns.values\n",
    "\n",
    "                    perturbed_table = pd.concat([table.loc[:insert_index], new_row, table.loc[insert_index:]]).reset_index(drop=True)\n",
    "            \n",
    "            return perturbed_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (table, text) in tqdm(enumerate(zip(original_tables, test_dataset.text_input)), position = 0, leave = True, total = len(test_dataset)):\n",
    "\n",
    "    # if i == 1:\n",
    "    perturbed_table = perturb_table(table=table, text_input=text, idx = i)\n",
    "    perturbed_tables.append(perturbed_table)\n",
    "    # display(table)\n",
    "    # display(perturbed_table)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"datasets/wiki_tq_test_perturbed_tables.pkl\", \"wb\") as f:\n",
    "    pickle.dump(perturbed_tables, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"configs/wiki_tq_reasoning/t5.json\", \"rb\") as f:\n",
    "    reason_generation_config = json.load(f)\n",
    "reason_generation_config = process_config(reason_generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_generation_config.training.training_type = \"table_question_answering\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.table = perturbed_tables\n",
    "test_dataset.config = reason_generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ModelForTableReasoning(reason_generation_config)\n",
    "model.load_state_dict(torch.load(\"Flan T5 Reason Generator/epoch=50.pt\", map_location = \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, table in enumerate(perturbed_tables):\n",
    "    table_column_names = table.columns.values.tolist()\n",
    "    table_content_values = table.values.tolist()\n",
    "    table = \"[HEADER] \" + \" | \".join(table_column_names)\n",
    "    for row_id, row in enumerate(table_content_values):\n",
    "        table += f\" [ROW] {row_id}: \" + \" | \".join(row)\n",
    "    \n",
    "    test_dataset.table[i] = table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_generations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "reason_tokenizer = AutoTokenizer.from_pretrained(reason_generation_config.tokenizer.tokenizer_path)\n",
    "if \"bos_token\" not in list(reason_tokenizer.special_tokens_map.keys()):\n",
    "    reason_tokenizer.add_special_tokens({\"bos_token\": \"<s>\"})\n",
    "\n",
    "if \"pad_token\" not in list(reason_tokenizer.special_tokens_map.keys()):\n",
    "    reason_tokenizer.add_special_tokens({\"pad_token\": reason_tokenizer.special_tokens_map[\"eos_token\"]})\n",
    "\n",
    "if \"sep_token\" not in list(reason_tokenizer.special_tokens_map.keys()):\n",
    "    reason_tokenizer.add_special_tokens({\"sep_token\": reason_tokenizer.special_tokens_map[\"eos_token\"]})\n",
    "\n",
    "if \"mask_token\" not in list(reason_tokenizer.special_tokens_map.keys()):\n",
    "    reason_tokenizer.add_special_tokens({\"mask_token\": reason_tokenizer.special_tokens_map[\"eos_token\"]})\n",
    "\n",
    "test_dataset.tokenizer = reason_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.config.data.use_highlighted_cells = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 24, shuffle = False, num_workers = config.system.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in tqdm(enumerate(test_dataloader), position = 0, leave = True, total = len(test_dataloader)):\n",
    "\n",
    "    input_ids, attention_mask, _, _, labels = batch\n",
    "    predicted_ids = model.model.generate(input_ids = input_ids.to(\"cuda:0\"), attention_mask = attention_mask.to(\"cuda:0\"), \n",
    "                                     max_new_tokens = config.tokenizer.output_max_length, num_beams = 3, early_stopping = True).detach().cpu()\n",
    "\n",
    "    batch_predicted_reason = test_dataset.tokenizer.batch_decode(predicted_ids, skip_special_tokens = True)\n",
    "\n",
    "    reason_generations.extend(batch_predicted_reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/wiki_tq_test_perturbed_tables_reasons.pkl\", \"wb\") as f:\n",
    "    pickle.dump(reason_generations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"configs/cell_highlighting/t5.json\", \"rb\") as f:\n",
    "    cell_highlighting_config = json.load(f)\n",
    "cell_highlighting_config = process_config(cell_highlighting_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ModelForTableCellHighlighting(cell_highlighting_config)\n",
    "model.load_state_dict(torch.load(\"Flan T5 highlighter/epoch=2.pt\", map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/wiki_tq_test_perturbed_tables_reasons.pkl\", \"rb\") as f:\n",
    "    reasons_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cell_highlighting_config.tokenizer.tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitablequestions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(idx):\n",
    "\n",
    "    reason = reasons_list[idx]\n",
    "\n",
    "    table = perturbed_tables[idx]\n",
    "    # table_column_names = table.columns.values\n",
    "    # table_content_values = table.values\n",
    "    # table_column_names = dataset[\"test\"][idx][\"table\"][\"header\"]\n",
    "    # table_content_values = dataset[\"test\"][idx][\"table\"][\"rows\"]\n",
    "\n",
    "    # table_column_names = [x.lower() for x in table_column_names]\n",
    "\n",
    "    # table = \"[HEADER] \" + \" | \".join(table_column_names)\n",
    "    # for row_id, row in enumerate(table_content_values):\n",
    "    #     row = [x.lower() for x in row]\n",
    "    #     table += f\" [ROW] {row_id}: \" + \" | \".join(row) \n",
    "\n",
    "    tokenized_input = tokenizer(reason, table, add_special_tokens = config.tokenizer.add_special_tokens,\n",
    "                            padding = config.tokenizer.padding, truncation = config.tokenizer.truncation, \n",
    "                            max_length = config.tokenizer.input_max_length, return_tensors = config.tokenizer.return_tensors,\n",
    "                            return_token_type_ids = config.tokenizer.return_token_type_ids,\n",
    "                            return_attention_mask = config.tokenizer.return_attention_mask)\n",
    "\n",
    "\n",
    "    output_ids = model.model.generate(input_ids = tokenized_input[\"input_ids\"].to(\"cuda:1\"), attention_mask = tokenized_input[\"attention_mask\"].to(\"cuda:1\"),\n",
    "                                      max_new_tokens = config.tokenizer.output_max_length, num_beams = 3, early_stopping = True).squeeze().detach().cpu()\n",
    "\n",
    "    predicted_cells = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "    return predicted_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlighted_cells_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(reasons_list)), position=0, leave = True, total = len(reasons_list)):\n",
    "    x = predict(i).split(\", \")\n",
    "    x = [a.strip() for a in x]\n",
    "    highlighted_cells_list.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/wiki_tq_test_highlighted_cell_perturbed_tables.pkl\", \"wb\") as f:\n",
    "    pickle.dump(highlighted_cells_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [x.strip() for x in perturbed_tables[0].split(\"[ROW]\")[0].split(\"[HEADER]\")[1].strip().split(\"|\")]\n",
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [[ele.strip() for ele in value.strip().split(\":\")[-1].strip().split(\"|\")] for value in perturbed_tables[2].split(\"[ROW]\")[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_tables_df = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pt in tqdm(perturbed_tables, position = 0, leave = True, total = len(perturbed_tables)):\n",
    "    table_column_names = [x.strip() for x in pt.split(\"[ROW]\")[0].split(\"[HEADER]\")[1].strip().split(\"|\")]\n",
    "    table_content_values = [[ele.strip() for ele in \" \".join(value.strip().split(\":\")[1:]).strip().split(\"|\")] for value in pt.split(\"[ROW]\")[1:]]\n",
    "\n",
    "    table_df = pd.DataFrame.from_dict({str(col).lower(): [str(table_content_values[j][i]).lower() for j in range(len(table_content_values))] for i, col in enumerate(table_column_names)})\n",
    "\n",
    "    perturbed_tables_df.append(table_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(perturbed_tables_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/wiki_tq_test_perturbed_tables.pkl\", \"wb\") as f:\n",
    "    pickle.dump(perturbed_tables_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/tapex-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "hard_relevance_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/wiki_tq_test_perturbed_tables.pkl\", \"rb\") as f:\n",
    "    perturbed_tables_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(test_dataset)), position = 0, leave = True, total = len(test_dataset)):\n",
    "\n",
    "    question = test_dataset[i][\"question\"]\n",
    "\n",
    "    table_column_names = perturbed_tables_df[i].columns.values\n",
    "    table_content_values = perturbed_tables_df[i].values\n",
    "\n",
    "    highlighted_cells = highlighted_cells_list[i]\n",
    "\n",
    "    # table = train_dataset[i][\"table\"]\n",
    "    # table_column_names = table[\"header\"]    \n",
    "    # table_content_values = table[\"rows\"]\n",
    "\n",
    "    table_df = pd.DataFrame.from_dict({str(col).lower(): [str(table_content_values[j][i]).lower() for j in range(len(table_content_values))] for i, col in enumerate(table_column_names)})\n",
    "    \n",
    "    tokenized_input = tokenizer(table_df, question, add_special_tokens = config.tokenizer.add_special_tokens,\n",
    "                            padding = config.tokenizer.padding, truncation = config.tokenizer.truncation, \n",
    "                            max_length = 960, return_tensors = config.tokenizer.return_tensors,\n",
    "                            return_token_type_ids = config.tokenizer.return_token_type_ids,\n",
    "                            return_attention_mask = config.tokenizer.return_attention_mask)\n",
    "    \n",
    "    tokenized_highlighted_cells = []\n",
    "    hard_relevance_label = torch.zeros((tokenized_input[\"input_ids\"].shape[1]))\n",
    "    for h_cell in highlighted_cells:\n",
    "        x = tokenizer(answer = h_cell, add_special_tokens = False,\n",
    "                            return_tensors = config.tokenizer.return_tensors,\n",
    "                            return_attention_mask = config.tokenizer.return_attention_mask)[\"input_ids\"].tolist()\n",
    "        for ele in x[0]:\n",
    "            hard_relevance_label[tokenized_input[\"input_ids\"].squeeze() == ele] = 1\n",
    "        \n",
    "    hard_relevance_labels.append(hard_relevance_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/wiki_tq_test_highlighted_cells_perturbed_table.pkl\", \"wb\") as f:\n",
    "    pickle.dump(hard_relevance_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/wiki_tq_test_highlighted_cells_perturbed_table_string.pkl\", \"wb\") as f:\n",
    "    pickle.dump(highlighted_cells, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on vanilla OmniTab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "from utils import process_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"configs/wiki_tq/tapex.json\", \"rb\") as f:\n",
    "    vanilla_qa_config = json.load(f)\n",
    "vanilla_qa_config = process_config(vanilla_qa_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"configs/wiki_tq_clustering_and_highlighting/tapex.json\", \"rb\") as f:\n",
    "    best_qa_config = json.load(f)\n",
    "best_qa_config = process_config(best_qa_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import BartModelForGenerativeQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import HighlightedCluBartModelForGenerativeQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BartModelForGenerativeQuestionAnswering(vanilla_qa_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HighlightedCluBartModelForGenerativeQuestionAnswering(best_qa_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"omnitab_best_ckpt/epoch=28.pt\", map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import WikiTQDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import WikiTQHighlightedCellsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"wikitablequestions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = WikiTQHighlightedCellsDataset(dataset=dataset, config = best_qa_config, data_type = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/wiki_tq_test_perturbed_tables.pkl\", \"rb\") as f:\n",
    "    perturbed_tables_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.table = perturbed_tables_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/wiki_tq_test_highlighted_cells_perturbed_table.pkl\", \"rb\") as f:\n",
    "    hard_relevance_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.highlighted_cells = hard_relevance_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_qa_config.data.use_highlighted_cells = False\n",
    "vanilla_qa_config.data.use_reason_in_output = False\n",
    "vanilla_qa_config.data.use_reason_in_input = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_test_dataset = WikiTQDataset(dataset = dataset, config = vanilla_qa_config, data_type = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_test_dataset.table = perturbed_tables_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import argparse\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from codecs import open\n",
    "from math import isnan, isinf\n",
    "from easydict import EasyDict\n",
    "from torch.utils.data import DataLoader\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "def normalize(x):\n",
    "\n",
    "    if not isinstance(x, str):\n",
    "        x = x.decode('utf8', errors='ignore')\n",
    "\n",
    "    # Remove diacritics\n",
    "    x = ''.join(c for c in unicodedata.normalize('NFKD', x)\n",
    "                if unicodedata.category(c) != 'Mn')\n",
    "    \n",
    "    # Normalize quotes and dashes\n",
    "    x = re.sub(r\"[‘’´`]\", \"'\", x)\n",
    "    x = re.sub(r\"[“”]\", \"\\\"\", x)\n",
    "    x = re.sub(r\"[‐‑‒–—−]\", \"-\", x)\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        old_x = x\n",
    "\n",
    "        # Remove citations\n",
    "        x = re.sub(r\"((?<!^)\\[[^\\]]*\\]|\\[\\d+\\]|[•♦†‡*#+])*$\", \"\", x.strip())\n",
    "        \n",
    "        # Remove details in parenthesis\n",
    "        x = re.sub(r\"(?<!^)( \\([^)]*\\))*$\", \"\", x.strip())\n",
    "        \n",
    "        # Remove outermost quotation mark\n",
    "        x = re.sub(r'^\"([^\"]*)\"$', r'\\1', x.strip())\n",
    "        \n",
    "        if x == old_x:\n",
    "            break\n",
    "    \n",
    "    # Remove final '.'\n",
    "    if x and x[-1] == '.':\n",
    "        x = x[:-1]\n",
    "    \n",
    "    # Collapse whitespaces and convert to lower case\n",
    "    x = re.sub(r'\\s+', ' ', x, flags=re.U).lower().strip()\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "class Value(object):\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    # Should be populated with the normalized string\n",
    "    _normalized = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def match(self, other):\n",
    "        \"\"\"Return True if the value matches the other value.\n",
    "\n",
    "        Args:\n",
    "            other (Value)\n",
    "        Returns:\n",
    "            a boolean\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def normalized(self):\n",
    "        return self._normalized\n",
    "\n",
    "\n",
    "class StringValue(Value):\n",
    "\n",
    "    def __init__(self, content):\n",
    "        assert isinstance(content, str)\n",
    "        self._normalized = normalize(content)\n",
    "        self._hash = hash(self._normalized)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, StringValue) and self.normalized == other.normalized\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self._hash\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'S' + str([self.normalized])\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "    def match(self, other):\n",
    "        assert isinstance(other, Value)\n",
    "        return self.normalized == other.normalized\n",
    "\n",
    "\n",
    "class NumberValue(Value):\n",
    "\n",
    "    def __init__(self, amount, original_string=None):\n",
    "        assert isinstance(amount, (int, float))\n",
    "        if abs(amount - round(amount)) < 1e-6:\n",
    "            self._amount = int(amount)\n",
    "        else:\n",
    "            self._amount = float(amount)\n",
    "        if not original_string:\n",
    "            self._normalized = str(self._amount)\n",
    "        else:\n",
    "            self._normalized = normalize(original_string)\n",
    "        self._hash = hash(self._amount)\n",
    "\n",
    "    @property\n",
    "    def amount(self):\n",
    "        return self._amount\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, NumberValue) and self.amount == other.amount\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self._hash\n",
    "\n",
    "    def __str__(self):\n",
    "        return ('N(%f)' % self.amount) + str([self.normalized])\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "    def match(self, other):\n",
    "        assert isinstance(other, Value)\n",
    "        if self.normalized == other.normalized:\n",
    "            return True\n",
    "        if isinstance(other, NumberValue):\n",
    "            return abs(self.amount - other.amount) < 1e-6\n",
    "        return False\n",
    "\n",
    "    @staticmethod\n",
    "    def parse(text):\n",
    "        \"\"\"Try to parse into a number.\n",
    "\n",
    "        Return:\n",
    "            the number (int or float) if successful; otherwise None.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return int(text)\n",
    "        except:\n",
    "            try:\n",
    "                amount = float(text)\n",
    "                assert not isnan(amount) and not isinf(amount)\n",
    "                return amount\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "\n",
    "class DateValue(Value):\n",
    "\n",
    "    def __init__(self, year, month, day, original_string=None):\n",
    "\n",
    "        \"\"\"Create a new DateValue. Placeholders are marked as -1.\"\"\"\n",
    "        assert isinstance(year, int)\n",
    "        assert isinstance(month, int) and (month == -1 or 1 <= month <= 12)\n",
    "        assert isinstance(day, int) and (day == -1 or 1 <= day <= 31)\n",
    "        assert not (year == month == day == -1)\n",
    "        \n",
    "        self._year = year\n",
    "        self._month = month\n",
    "        self._day = day\n",
    "        \n",
    "        if not original_string:\n",
    "            self._normalized = '{}-{}-{}'.format(\n",
    "                year if year != -1 else 'xx',\n",
    "                month if month != -1 else 'xx',\n",
    "                day if day != '-1' else 'xx')\n",
    "        else:\n",
    "            self._normalized = normalize(original_string)\n",
    "        \n",
    "        self._hash = hash((self._year, self._month, self._day))\n",
    "\n",
    "    @property\n",
    "    def ymd(self):\n",
    "        return (self._year, self._month, self._day)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, DateValue) and self.ymd == other.ymd\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self._hash\n",
    "\n",
    "    def __str__(self):\n",
    "        return (('D(%d,%d,%d)' % (self._year, self._month, self._day))\n",
    "                + str([self._normalized]))\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "    def match(self, other):\n",
    "        \n",
    "        assert isinstance(other, Value)\n",
    "        \n",
    "        if self.normalized == other.normalized:\n",
    "            return True\n",
    "        \n",
    "        if isinstance(other, DateValue):\n",
    "            return self.ymd == other.ymd\n",
    "        \n",
    "        return False\n",
    "\n",
    "    @staticmethod\n",
    "    def parse(text):\n",
    "        \"\"\"Try to parse into a date.\n",
    "\n",
    "        Return:\n",
    "            tuple (year, month, date) if successful; otherwise None.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            ymd = text.lower().split('-')\n",
    "            assert len(ymd) == 3\n",
    "            year = -1 if ymd[0] in ('xx', 'xxxx') else int(ymd[0])\n",
    "            month = -1 if ymd[1] == 'xx' else int(ymd[1])\n",
    "            day = -1 if ymd[2] == 'xx' else int(ymd[2])\n",
    "            assert not (year == month == day == -1)\n",
    "            assert month == -1 or 1 <= month <= 12\n",
    "            assert day == -1 or 1 <= day <= 31\n",
    "            return (year, month, day)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "def to_value(original_string, corenlp_value=None):\n",
    "    \"\"\"Convert the string to Value object.\n",
    "\n",
    "    Args:\n",
    "        original_string (basestring): Original string\n",
    "        corenlp_value (basestring): Optional value returned from CoreNLP\n",
    "    Returns:\n",
    "        Value\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(original_string, Value):\n",
    "        # Already a Value\n",
    "        return original_string\n",
    "    \n",
    "    if not corenlp_value:\n",
    "        corenlp_value = original_string\n",
    "    \n",
    "    # Number?\n",
    "    amount = NumberValue.parse(corenlp_value)\n",
    "    \n",
    "    if amount is not None:\n",
    "        return NumberValue(amount, original_string)\n",
    "    \n",
    "    # Date?\n",
    "    ymd = DateValue.parse(corenlp_value)\n",
    "    if ymd is not None:\n",
    "        if ymd[1] == ymd[2] == -1:\n",
    "            return NumberValue(ymd[0], original_string)\n",
    "        else:\n",
    "            return DateValue(ymd[0], ymd[1], ymd[2], original_string)\n",
    "    \n",
    "    # String.\n",
    "    return StringValue(original_string)\n",
    "\n",
    "\n",
    "def to_value_list(original_strings, corenlp_values=None):\n",
    "    \"\"\"Convert a list of strings to a list of Values\n",
    "\n",
    "    Args:\n",
    "        original_strings (list[basestring])\n",
    "        corenlp_values (list[basestring or None])\n",
    "    Returns:\n",
    "        list[Value]\n",
    "    \"\"\"\n",
    "    assert isinstance(original_strings, (list, tuple, set))\n",
    "    if corenlp_values is not None:\n",
    "        assert isinstance(corenlp_values, (list, tuple, set))\n",
    "        assert len(original_strings) == len(corenlp_values)\n",
    "        return list(set(to_value(x, y) for (x, y)\n",
    "                        in zip(original_strings, corenlp_values)))\n",
    "    else:\n",
    "        return list(set(to_value(x) for x in original_strings))\n",
    "\n",
    "\n",
    "def check_denotation(target_values, predicted_values):\n",
    "    \"\"\"Return True if the predicted denotation is correct.\n",
    "\n",
    "    Args:\n",
    "        target_values (list[Value])\n",
    "        predicted_values (list[Value])\n",
    "    Returns:\n",
    "        bool\n",
    "    \"\"\"\n",
    "\n",
    "    # Check size\n",
    "    if len(target_values) != len(predicted_values):\n",
    "        return False\n",
    "    \n",
    "    # Check items\n",
    "    for target in target_values:\n",
    "        if not any(target.match(pred) for pred in predicted_values):\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def tsv_unescape(x):\n",
    "    \"\"\"Unescape strings in the TSV file.\n",
    "    Escaped characters include:\n",
    "        newline (0x10) -> backslash + n\n",
    "        vertical bar (0x7C) -> backslash + p\n",
    "        backslash (0x5C) -> backslash + backslash\n",
    "\n",
    "    Args:\n",
    "        x (str or unicode)\n",
    "    Returns:\n",
    "        a unicode\n",
    "    \"\"\"\n",
    "    return x.replace(r'\\n', '\\n').replace(r'\\p', '|').replace('\\\\\\\\', '\\\\')\n",
    "\n",
    "\n",
    "def tsv_unescape_list(x):\n",
    "    \"\"\"Unescape a list in the TSV file.\n",
    "    List items are joined with vertical bars (0x5C)\n",
    "\n",
    "    Args:\n",
    "        x (str or unicode)\n",
    "    Returns:\n",
    "        a list of unicodes\n",
    "    \"\"\"\n",
    "    return [tsv_unescape(y) for y in x.split('|')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda:2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = test_dataset.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(index):\n",
    "\n",
    "    input_ids, attention_mask, token_type_ids, decoder_input_ids, highlighted_cells, labels = test_dataset.__getitem__(index)\n",
    "    output_ids = model.model.generate(input_ids = input_ids.unsqueeze(0).to(\"cuda:2\"), max_new_tokens = vanilla_qa_config.tokenizer.output_max_length, \n",
    "                                    num_beams = 3, early_stopping = True, attention_mask = attention_mask.unsqueeze(0).to(\"cuda:2\"),\n",
    "                                    highlighted_cells=highlighted_cells.unsqueeze(0).to(\"cuda:2\")).detach().cpu().squeeze()\n",
    "    \n",
    "    p = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    a = tokenizer.decode(decoder_input_ids, skip_special_tokens = True)\n",
    "\n",
    "    a = [x.strip().lower() for x in a.split(\",\")]\n",
    "    p = [x.strip() for x in p.split(\",\")]\n",
    "\n",
    "    pred = to_value_list(p)\n",
    "    gold = to_value_list(a)\n",
    "\n",
    "    verdict = check_denotation(gold, pred)\n",
    "\n",
    "    return verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(test_dataset)), position=0, leave = True, total = len(test_dataset)):\n",
    "    verdict = predict(i)\n",
    "    if verdict:\n",
    "        correct += 1\n",
    "    total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random row permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "from src import BartModelForGenerativeQuestionAnswering\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitablequestions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import WikiTQDataset\n",
    "from utils import process_config\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"configs/wiki_tq/tapex.json\", \"rb\") as f:\n",
    "    config = json.load(f)\n",
    "config = process_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = WikiTQDataset(dataset=dataset, config=config, data_type=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_dataset.table)):\n",
    "    test_dataset.table[i] = test_dataset.table[i].sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = test_dataset.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BartModelForGenerativeQuestionAnswering(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import argparse\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from codecs import open\n",
    "from math import isnan, isinf\n",
    "from easydict import EasyDict\n",
    "from torch.utils.data import DataLoader\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "def normalize(x):\n",
    "\n",
    "    if not isinstance(x, str):\n",
    "        x = x.decode('utf8', errors='ignore')\n",
    "\n",
    "    # Remove diacritics\n",
    "    x = ''.join(c for c in unicodedata.normalize('NFKD', x)\n",
    "                if unicodedata.category(c) != 'Mn')\n",
    "    \n",
    "    # Normalize quotes and dashes\n",
    "    x = re.sub(r\"[‘’´`]\", \"'\", x)\n",
    "    x = re.sub(r\"[“”]\", \"\\\"\", x)\n",
    "    x = re.sub(r\"[‐‑‒–—−]\", \"-\", x)\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        old_x = x\n",
    "\n",
    "        # Remove citations\n",
    "        x = re.sub(r\"((?<!^)\\[[^\\]]*\\]|\\[\\d+\\]|[•♦†‡*#+])*$\", \"\", x.strip())\n",
    "        \n",
    "        # Remove details in parenthesis\n",
    "        x = re.sub(r\"(?<!^)( \\([^)]*\\))*$\", \"\", x.strip())\n",
    "        \n",
    "        # Remove outermost quotation mark\n",
    "        x = re.sub(r'^\"([^\"]*)\"$', r'\\1', x.strip())\n",
    "        \n",
    "        if x == old_x:\n",
    "            break\n",
    "    \n",
    "    # Remove final '.'\n",
    "    if x and x[-1] == '.':\n",
    "        x = x[:-1]\n",
    "    \n",
    "    # Collapse whitespaces and convert to lower case\n",
    "    x = re.sub(r'\\s+', ' ', x, flags=re.U).lower().strip()\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "class Value(object):\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    # Should be populated with the normalized string\n",
    "    _normalized = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def match(self, other):\n",
    "        \"\"\"Return True if the value matches the other value.\n",
    "\n",
    "        Args:\n",
    "            other (Value)\n",
    "        Returns:\n",
    "            a boolean\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def normalized(self):\n",
    "        return self._normalized\n",
    "\n",
    "\n",
    "class StringValue(Value):\n",
    "\n",
    "    def __init__(self, content):\n",
    "        assert isinstance(content, str)\n",
    "        self._normalized = normalize(content)\n",
    "        self._hash = hash(self._normalized)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, StringValue) and self.normalized == other.normalized\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self._hash\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'S' + str([self.normalized])\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "    def match(self, other):\n",
    "        assert isinstance(other, Value)\n",
    "        return self.normalized == other.normalized\n",
    "\n",
    "\n",
    "class NumberValue(Value):\n",
    "\n",
    "    def __init__(self, amount, original_string=None):\n",
    "        assert isinstance(amount, (int, float))\n",
    "        if abs(amount - round(amount)) < 1e-6:\n",
    "            self._amount = int(amount)\n",
    "        else:\n",
    "            self._amount = float(amount)\n",
    "        if not original_string:\n",
    "            self._normalized = str(self._amount)\n",
    "        else:\n",
    "            self._normalized = normalize(original_string)\n",
    "        self._hash = hash(self._amount)\n",
    "\n",
    "    @property\n",
    "    def amount(self):\n",
    "        return self._amount\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, NumberValue) and self.amount == other.amount\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self._hash\n",
    "\n",
    "    def __str__(self):\n",
    "        return ('N(%f)' % self.amount) + str([self.normalized])\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "    def match(self, other):\n",
    "        assert isinstance(other, Value)\n",
    "        if self.normalized == other.normalized:\n",
    "            return True\n",
    "        if isinstance(other, NumberValue):\n",
    "            return abs(self.amount - other.amount) < 1e-6\n",
    "        return False\n",
    "\n",
    "    @staticmethod\n",
    "    def parse(text):\n",
    "        \"\"\"Try to parse into a number.\n",
    "\n",
    "        Return:\n",
    "            the number (int or float) if successful; otherwise None.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return int(text)\n",
    "        except:\n",
    "            try:\n",
    "                amount = float(text)\n",
    "                assert not isnan(amount) and not isinf(amount)\n",
    "                return amount\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "\n",
    "class DateValue(Value):\n",
    "\n",
    "    def __init__(self, year, month, day, original_string=None):\n",
    "\n",
    "        \"\"\"Create a new DateValue. Placeholders are marked as -1.\"\"\"\n",
    "        assert isinstance(year, int)\n",
    "        assert isinstance(month, int) and (month == -1 or 1 <= month <= 12)\n",
    "        assert isinstance(day, int) and (day == -1 or 1 <= day <= 31)\n",
    "        assert not (year == month == day == -1)\n",
    "        \n",
    "        self._year = year\n",
    "        self._month = month\n",
    "        self._day = day\n",
    "        \n",
    "        if not original_string:\n",
    "            self._normalized = '{}-{}-{}'.format(\n",
    "                year if year != -1 else 'xx',\n",
    "                month if month != -1 else 'xx',\n",
    "                day if day != '-1' else 'xx')\n",
    "        else:\n",
    "            self._normalized = normalize(original_string)\n",
    "        \n",
    "        self._hash = hash((self._year, self._month, self._day))\n",
    "\n",
    "    @property\n",
    "    def ymd(self):\n",
    "        return (self._year, self._month, self._day)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, DateValue) and self.ymd == other.ymd\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self._hash\n",
    "\n",
    "    def __str__(self):\n",
    "        return (('D(%d,%d,%d)' % (self._year, self._month, self._day))\n",
    "                + str([self._normalized]))\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "    def match(self, other):\n",
    "        \n",
    "        assert isinstance(other, Value)\n",
    "        \n",
    "        if self.normalized == other.normalized:\n",
    "            return True\n",
    "        \n",
    "        if isinstance(other, DateValue):\n",
    "            return self.ymd == other.ymd\n",
    "        \n",
    "        return False\n",
    "\n",
    "    @staticmethod\n",
    "    def parse(text):\n",
    "        \"\"\"Try to parse into a date.\n",
    "\n",
    "        Return:\n",
    "            tuple (year, month, date) if successful; otherwise None.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            ymd = text.lower().split('-')\n",
    "            assert len(ymd) == 3\n",
    "            year = -1 if ymd[0] in ('xx', 'xxxx') else int(ymd[0])\n",
    "            month = -1 if ymd[1] == 'xx' else int(ymd[1])\n",
    "            day = -1 if ymd[2] == 'xx' else int(ymd[2])\n",
    "            assert not (year == month == day == -1)\n",
    "            assert month == -1 or 1 <= month <= 12\n",
    "            assert day == -1 or 1 <= day <= 31\n",
    "            return (year, month, day)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "def to_value(original_string, corenlp_value=None):\n",
    "    \"\"\"Convert the string to Value object.\n",
    "\n",
    "    Args:\n",
    "        original_string (basestring): Original string\n",
    "        corenlp_value (basestring): Optional value returned from CoreNLP\n",
    "    Returns:\n",
    "        Value\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(original_string, Value):\n",
    "        # Already a Value\n",
    "        return original_string\n",
    "    \n",
    "    if not corenlp_value:\n",
    "        corenlp_value = original_string\n",
    "    \n",
    "    # Number?\n",
    "    amount = NumberValue.parse(corenlp_value)\n",
    "    \n",
    "    if amount is not None:\n",
    "        return NumberValue(amount, original_string)\n",
    "    \n",
    "    # Date?\n",
    "    ymd = DateValue.parse(corenlp_value)\n",
    "    if ymd is not None:\n",
    "        if ymd[1] == ymd[2] == -1:\n",
    "            return NumberValue(ymd[0], original_string)\n",
    "        else:\n",
    "            return DateValue(ymd[0], ymd[1], ymd[2], original_string)\n",
    "    \n",
    "    # String.\n",
    "    return StringValue(original_string)\n",
    "\n",
    "\n",
    "def to_value_list(original_strings, corenlp_values=None):\n",
    "    \"\"\"Convert a list of strings to a list of Values\n",
    "\n",
    "    Args:\n",
    "        original_strings (list[basestring])\n",
    "        corenlp_values (list[basestring or None])\n",
    "    Returns:\n",
    "        list[Value]\n",
    "    \"\"\"\n",
    "    assert isinstance(original_strings, (list, tuple, set))\n",
    "    if corenlp_values is not None:\n",
    "        assert isinstance(corenlp_values, (list, tuple, set))\n",
    "        assert len(original_strings) == len(corenlp_values)\n",
    "        return list(set(to_value(x, y) for (x, y)\n",
    "                        in zip(original_strings, corenlp_values)))\n",
    "    else:\n",
    "        return list(set(to_value(x) for x in original_strings))\n",
    "\n",
    "\n",
    "def check_denotation(target_values, predicted_values):\n",
    "    \"\"\"Return True if the predicted denotation is correct.\n",
    "\n",
    "    Args:\n",
    "        target_values (list[Value])\n",
    "        predicted_values (list[Value])\n",
    "    Returns:\n",
    "        bool\n",
    "    \"\"\"\n",
    "\n",
    "    # Check size\n",
    "    if len(target_values) != len(predicted_values):\n",
    "        return False\n",
    "    \n",
    "    # Check items\n",
    "    for target in target_values:\n",
    "        if not any(target.match(pred) for pred in predicted_values):\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def tsv_unescape(x):\n",
    "    \"\"\"Unescape strings in the TSV file.\n",
    "    Escaped characters include:\n",
    "        newline (0x10) -> backslash + n\n",
    "        vertical bar (0x7C) -> backslash + p\n",
    "        backslash (0x5C) -> backslash + backslash\n",
    "\n",
    "    Args:\n",
    "        x (str or unicode)\n",
    "    Returns:\n",
    "        a unicode\n",
    "    \"\"\"\n",
    "    return x.replace(r'\\n', '\\n').replace(r'\\p', '|').replace('\\\\\\\\', '\\\\')\n",
    "\n",
    "\n",
    "def tsv_unescape_list(x):\n",
    "    \"\"\"Unescape a list in the TSV file.\n",
    "    List items are joined with vertical bars (0x5C)\n",
    "\n",
    "    Args:\n",
    "        x (str or unicode)\n",
    "    Returns:\n",
    "        a list of unicodes\n",
    "    \"\"\"\n",
    "    return [tsv_unescape(y) for y in x.split('|')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verdict(index):\n",
    "\n",
    "    input_ids, attention_mask, token_type_ids, decoder_input_ids, labels = test_dataset.__getitem__(index)\n",
    "    output_ids = model.model.generate(input_ids = input_ids.unsqueeze(0).to(\"cuda:0\"), max_new_tokens = config.tokenizer.output_max_length, \n",
    "                                    num_beams = 3, early_stopping = True, attention_mask = attention_mask.unsqueeze(0).to(\"cuda:0\")).detach().cpu().squeeze()\n",
    "    \n",
    "    p = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    a = tokenizer.decode(decoder_input_ids, skip_special_tokens = True)\n",
    "\n",
    "    a = [x.strip().lower() for x in a.split(\",\")]\n",
    "    p = [x.strip() for x in p.split(\",\")]\n",
    "\n",
    "    pred = to_value_list(p)\n",
    "    gold = to_value_list(a)\n",
    "\n",
    "    verdict = check_denotation(gold, pred)\n",
    "\n",
    "    return verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(test_dataset)), position = 0, leave = True):\n",
    "    verdict = get_verdict(i)\n",
    "\n",
    "    if verdict:\n",
    "        correct += 1\n",
    "    total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radar Plot for WikiTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the categories (perturbation types)\n",
    "categories = ['Row addition', 'Row permutation', 'Column permutation', 'Cell replacement', 'Row deletion']\n",
    "\n",
    "# Define the data for the baseline and novel proposed models for each category\n",
    "\n",
    "baseline_performance = 62.730\n",
    "our_method_performance = 69.130\n",
    "\n",
    "baseline_perturbed_performance = np.array([50.875, 57.804, 59.611, 56.256, 54.619])\n",
    "our_method_perturbed_performance = np.array([61.205, 67.380, 68.116, 66.096, 64.778])\n",
    "\n",
    "baseline_data = list(baseline_performance - baseline_perturbed_performance)\n",
    "proposed_data = list(our_method_performance - our_method_perturbed_performance)\n",
    "\n",
    "# baseline_data = [0.8, 0.6, 0.7, 0.5, 0.9]  # Example data for the baseline model\n",
    "# proposed_data = [0.6, 0.8, 0.9, 0.7, 0.5]  # Example data for the novel proposed model\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "\n",
    "# Number of categories\n",
    "num_categories = len(categories)\n",
    "\n",
    "# Create an array of angles (in radians) for each category\n",
    "angles = np.linspace(0, 2 * np.pi, num_categories, endpoint=False).tolist()\n",
    "\n",
    "# Make the plot circular\n",
    "angles += angles[:1]\n",
    "\n",
    "# Initialize the figure and axes\n",
    "ax = plt.subplot(111, polar=True)\n",
    "\n",
    "# Plot the data as a filled area\n",
    "ax.fill(angles[:-1], baseline_data, 'b', alpha=0.2, c = \"red\")\n",
    "\n",
    "# Set the labels for each axis\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories)\n",
    "\n",
    "\n",
    "# Number of categories\n",
    "num_categories = len(categories)\n",
    "\n",
    "# Create an array of angles (in radians) for each category\n",
    "angles = np.linspace(0, 2 * np.pi, num_categories, endpoint=False).tolist()\n",
    "\n",
    "# Make the plot circular\n",
    "angles += angles[:1]\n",
    "\n",
    "# Initialize the figure and axes\n",
    "ax = plt.subplot(111, polar=True)\n",
    "\n",
    "# Plot the data as a filled area\n",
    "ax.fill(angles[:-1], proposed_data, 'b', alpha=0.1, c = \"blue\")\n",
    "\n",
    "# Set the labels for each axis\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories)\n",
    "\n",
    "# Create a function to plot radar charts\n",
    "# def plot_radar_chart(data, model_name):\n",
    "#     # Number of categories\n",
    "#     num_categories = len(categories)\n",
    "\n",
    "#     # Create an array of angles (in radians) for each category\n",
    "#     angles = np.linspace(0, 2 * np.pi, num_categories, endpoint=False).tolist()\n",
    "\n",
    "#     # Make the plot circular\n",
    "#     angles += angles[:1]\n",
    "\n",
    "#     # Initialize the figure and axes\n",
    "#     ax = plt.subplot(111, polar=True)\n",
    "\n",
    "#     # Plot the data as a filled area\n",
    "#     ax.fill(angles[:-1], data, 'b', alpha=0.2, c = \"red\")\n",
    "\n",
    "#     # Set the labels for each axis\n",
    "#     ax.set_xticks(angles[:-1])\n",
    "#     ax.set_xticklabels(categories)\n",
    "\n",
    "#     # Set the title\n",
    "#     # plt.title(f'Performance Comparison ({model_name})')\n",
    "\n",
    "# Plot radar charts for the baseline and novel proposed models\n",
    "# plot_radar_chart(baseline_data, 'OmniTab')\n",
    "# plot_radar_chart(proposed_data, 'Our Method')\n",
    "\n",
    "# Show the radar charts\n",
    "plt.legend(['Baseline Model', 'Our Method'], loc='lower left')\n",
    "# plt.title(f'Drop in denotation accuracy on WikiTQ')\n",
    "plt.savefig(\"wikitq_radar_plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FeTaQA Radar Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the categories (perturbation types)\n",
    "categories = ['Row addition', 'Row permutation', 'Column permutation', 'Cell replacement', 'Row deletion']\n",
    "\n",
    "# Define the data for the baseline and novel proposed models for each category\n",
    "\n",
    "baseline_performance = 34.890\n",
    "our_method_performance = 40.532\n",
    "\n",
    "baseline_perturbed_performance = np.array([27.353, 31.812, 30.732, 33.191, 31.740])\n",
    "our_method_perturbed_performance = np.array([35.503, 38.898, 37.523, 39.591, 37.818])\n",
    "\n",
    "baseline_data = list(baseline_performance - baseline_perturbed_performance)\n",
    "proposed_data = list(our_method_performance - our_method_perturbed_performance)\n",
    "\n",
    "# baseline_data = [0.8, 0.6, 0.7, 0.5, 0.9]  # Example data for the baseline model\n",
    "# proposed_data = [0.6, 0.8, 0.9, 0.7, 0.5]  # Example data for the novel proposed model\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "\n",
    "# Number of categories\n",
    "num_categories = len(categories)\n",
    "\n",
    "# Create an array of angles (in radians) for each category\n",
    "angles = np.linspace(0, 2 * np.pi, num_categories, endpoint=False).tolist()\n",
    "\n",
    "# Make the plot circular\n",
    "angles += angles[:1]\n",
    "\n",
    "# Initialize the figure and axes\n",
    "ax = plt.subplot(111, polar=True)\n",
    "\n",
    "# Plot the data as a filled area\n",
    "ax.fill(angles[:-1], baseline_data, 'b', alpha=0.2, c = \"red\")\n",
    "\n",
    "# Set the labels for each axis\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories)\n",
    "\n",
    "\n",
    "# Number of categories\n",
    "num_categories = len(categories)\n",
    "\n",
    "# Create an array of angles (in radians) for each category\n",
    "angles = np.linspace(0, 2 * np.pi, num_categories, endpoint=False).tolist()\n",
    "\n",
    "# Make the plot circular\n",
    "angles += angles[:1]\n",
    "\n",
    "# Initialize the figure and axes\n",
    "ax = plt.subplot(111, polar=True)\n",
    "\n",
    "# Plot the data as a filled area\n",
    "ax.fill(angles[:-1], proposed_data, 'b', alpha=0.1, c = \"blue\")\n",
    "\n",
    "# Set the labels for each axis\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories)\n",
    "\n",
    "\n",
    "# Create a function to plot radar charts\n",
    "# def plot_radar_chart(data, model_name):\n",
    "#     # Number of categories\n",
    "#     num_categories = len(categories)\n",
    "\n",
    "#     # Create an array of angles (in radians) for each category\n",
    "#     angles = np.linspace(0, 2 * np.pi, num_categories, endpoint=False).tolist()\n",
    "\n",
    "#     # Make the plot circular\n",
    "#     angles += angles[:1]\n",
    "\n",
    "#     # Initialize the figure and axes\n",
    "#     ax = plt.subplot(111, polar=True)\n",
    "\n",
    "#     # Plot the data as a filled area\n",
    "#     ax.fill(angles[:-1], data, 'b', alpha=0.2, c = \"green\")\n",
    "\n",
    "#     # Set the labels for each axis\n",
    "#     ax.set_xticks(angles[:-1])\n",
    "#     ax.set_xticklabels(categories)\n",
    "\n",
    "#     # Set the title\n",
    "#     # plt.title(f'Performance Comparison ({model_name})')\n",
    "\n",
    "# # Plot radar charts for the baseline and novel proposed models\n",
    "# plot_radar_chart(baseline_data, 'OmniTab')\n",
    "# plot_radar_chart(proposed_data, 'Our Method')\n",
    "\n",
    "# Show the radar charts\n",
    "plt.legend(['Baseline Model', 'Novel Proposed Model'], loc='lower left')\n",
    "# plt.title(f'Drop in denotation accuracy on WikiTQ')\n",
    "plt.savefig(\"fetaqa_radar_plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the categories (perturbation types)\n",
    "categories = ['Row addition', 'Row permutation', 'Column permutation', 'Cell replacement', 'Row deletion']\n",
    "\n",
    "# Define the data for the baseline and novel proposed models for each category\n",
    "\n",
    "baseline_performance = 87.911\n",
    "our_method_performance = 89.180\n",
    "\n",
    "baseline_perturbed_performance = np.array([81.325, 83.819, 83.367, 83.495, 84.263])\n",
    "our_method_perturbed_performance = np.array([87.264, 88.161, 87.406, 88.228, 87.730])\n",
    "\n",
    "baseline_data = list(baseline_performance - baseline_perturbed_performance)\n",
    "proposed_data = list(our_method_performance - our_method_perturbed_performance)\n",
    "\n",
    "# baseline_data = [0.8, 0.6, 0.7, 0.5, 0.9]  # Example data for the baseline model\n",
    "# proposed_data = [0.6, 0.8, 0.9, 0.7, 0.5]  # Example data for the novel proposed model\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "\n",
    "# Number of categories\n",
    "num_categories = len(categories)\n",
    "\n",
    "# Create an array of angles (in radians) for each category\n",
    "angles = np.linspace(0, 2 * np.pi, num_categories, endpoint=False).tolist()\n",
    "\n",
    "# Make the plot circular\n",
    "angles += angles[:1]\n",
    "\n",
    "# Initialize the figure and axes\n",
    "ax = plt.subplot(111, polar=True)\n",
    "\n",
    "# Plot the data as a filled area\n",
    "ax.fill(angles[:-1], baseline_data, 'b', alpha=0.2, c = \"red\")\n",
    "\n",
    "# Set the labels for each axis\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories)\n",
    "\n",
    "\n",
    "# Number of categories\n",
    "num_categories = len(categories)\n",
    "\n",
    "# Create an array of angles (in radians) for each category\n",
    "angles = np.linspace(0, 2 * np.pi, num_categories, endpoint=False).tolist()\n",
    "\n",
    "# Make the plot circular\n",
    "angles += angles[:1]\n",
    "\n",
    "# Initialize the figure and axes\n",
    "ax = plt.subplot(111, polar=True)\n",
    "\n",
    "# Plot the data as a filled area\n",
    "ax.fill(angles[:-1], proposed_data, 'b', alpha=0.1, c = \"blue\")\n",
    "\n",
    "# Set the labels for each axis\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories)\n",
    "\n",
    "\n",
    "# Create a function to plot radar charts\n",
    "# def plot_radar_chart(data, model_name):\n",
    "#     # Number of categories\n",
    "#     num_categories = len(categories)\n",
    "\n",
    "#     # Create an array of angles (in radians) for each category\n",
    "#     angles = np.linspace(0, 2 * np.pi, num_categories, endpoint=False).tolist()\n",
    "\n",
    "#     # Make the plot circular\n",
    "#     angles += angles[:1]\n",
    "\n",
    "#     # Initialize the figure and axes\n",
    "#     ax = plt.subplot(111, polar=True)\n",
    "\n",
    "#     # Plot the data as a filled area\n",
    "#     ax.fill(angles[:-1], data, 'b', alpha=0.2, c = \"green\")\n",
    "\n",
    "#     # Set the labels for each axis\n",
    "#     ax.set_xticks(angles[:-1])\n",
    "#     ax.set_xticklabels(categories)\n",
    "\n",
    "#     # Set the title\n",
    "#     # plt.title(f'Performance Comparison ({model_name})')\n",
    "\n",
    "# # Plot radar charts for the baseline and novel proposed models\n",
    "# plot_radar_chart(baseline_data, 'OmniTab')\n",
    "# plot_radar_chart(proposed_data, 'Our Method')\n",
    "\n",
    "# Show the radar charts\n",
    "plt.legend(['Baseline Model', 'Novel Proposed Model'], loc='lower left')\n",
    "# plt.title(f'Drop in denotation accuracy on WikiTQ')\n",
    "plt.savefig(\"wikisql_radar_plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bart Plot for WikiTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the categories (perturbation types)\n",
    "categories = ['Row addition', 'Row permutation', 'Column permutation', 'Cell replacement', 'Row deletion']\n",
    "\n",
    "# Define the data for the baseline and novel proposed models for each category\n",
    "\n",
    "baseline_performance = 62.730\n",
    "our_method_performance = 69.130\n",
    "\n",
    "baseline_perturbed_performance = np.array([50.875, 57.804, 59.611, 56.256, 54.619])\n",
    "our_method_perturbed_performance = np.array([61.205, 67.380, 68.116, 66.096, 64.778])\n",
    "\n",
    "baseline_data = list(baseline_performance - baseline_perturbed_performance)\n",
    "proposed_data = list(our_method_performance - our_method_perturbed_performance)\n",
    "\n",
    "# Number of categories\n",
    "num_categories = len(categories)\n",
    "\n",
    "# Create an array of indices for the categories\n",
    "x = np.arange(num_categories)\n",
    "\n",
    "# Width of each bar\n",
    "bar_width = 0.2\n",
    "\n",
    "# Create the bar plots for baseline and proposed models\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(x - bar_width/2, baseline_data, bar_width, label='OmniTab', alpha=0.7)\n",
    "plt.bar(x + bar_width/2, proposed_data, bar_width, label='Our Method', alpha=0.7)\n",
    "\n",
    "# Set the x-axis labels\n",
    "# plt.xlabel('Perturbation Types')\n",
    "plt.xticks(x, categories)\n",
    "\n",
    "# Set the y-axis label and title\n",
    "plt.ylabel('Drop in performance')\n",
    "# plt.title('Performance Comparison Across Perturbation Types')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the bar plot\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"wikitq_bar_plot.png\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the categories (perturbation types)\n",
    "categories = ['Row addition', 'Row permutation', 'Column permutation', 'Cell replacement', 'Row deletion']\n",
    "\n",
    "# Define the data for the baseline and novel proposed models for each category\n",
    "\n",
    "baseline_performance = 34.890\n",
    "our_method_performance = 40.532\n",
    "\n",
    "baseline_perturbed_performance = np.array([27.353, 31.812, 30.732, 33.191, 31.740])\n",
    "our_method_perturbed_performance = np.array([35.503, 38.898, 37.523, 39.591, 37.818])\n",
    "\n",
    "baseline_data = list(baseline_performance - baseline_perturbed_performance)\n",
    "proposed_data = list(our_method_performance - our_method_perturbed_performance)\n",
    "\n",
    "# Number of categories\n",
    "num_categories = len(categories)\n",
    "\n",
    "# Create an array of indices for the categories\n",
    "x = np.arange(num_categories)\n",
    "\n",
    "# Width of each bar\n",
    "bar_width = 0.2\n",
    "\n",
    "# Create the bar plots for baseline and proposed models\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(x - bar_width/2, baseline_data, bar_width, label='OmniTab', alpha=0.7)\n",
    "plt.bar(x + bar_width/2, proposed_data, bar_width, label='Our Method', alpha=0.7)\n",
    "\n",
    "# Set the x-axis labels\n",
    "# plt.xlabel('Perturbation Types')\n",
    "plt.xticks(x, categories)\n",
    "\n",
    "# Set the y-axis label and title\n",
    "plt.ylabel('Drop in performance')\n",
    "# plt.title('Performance Comparison Across Perturbation Types')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the bar plot\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"fetaqa_bar_plot.png\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the categories (perturbation types)\n",
    "categories = ['Row addition', 'Row permutation', 'Column permutation', 'Cell replacement', 'Row deletion']\n",
    "\n",
    "# Define the data for the baseline and novel proposed models for each category\n",
    "\n",
    "baseline_performance = 87.911\n",
    "our_method_performance = 89.180\n",
    "\n",
    "baseline_perturbed_performance = np.array([81.325, 83.819, 83.367, 83.495, 84.263])\n",
    "our_method_perturbed_performance = np.array([87.264, 88.161, 87.406, 88.228, 87.730])\n",
    "\n",
    "baseline_data = list(baseline_performance - baseline_perturbed_performance)\n",
    "proposed_data = list(our_method_performance - our_method_perturbed_performance)\n",
    "\n",
    "# Number of categories\n",
    "num_categories = len(categories)\n",
    "\n",
    "# Create an array of indices for the categories\n",
    "x = np.arange(num_categories)\n",
    "\n",
    "# Width of each bar\n",
    "bar_width = 0.2\n",
    "\n",
    "# Create the bar plots for baseline and proposed models\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(x - bar_width/2, baseline_data, bar_width, label='OmniTab', alpha=0.7)\n",
    "plt.bar(x + bar_width/2, proposed_data, bar_width, label='Our Method', alpha=0.7)\n",
    "\n",
    "# Set the x-axis labels\n",
    "# plt.xlabel('Perturbation Types')\n",
    "plt.xticks(x, categories)\n",
    "\n",
    "# Set the y-axis label and title\n",
    "plt.ylabel('Drop in performance')\n",
    "# plt.title('Performance Comparison Across Perturbation Types')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the bar plot\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"wikisql_bar_plot.png\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams['font.weight'] = 'bold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the categories (perturbation types)\n",
    "categories = ['Row addition', 'Row permutation', 'Column permutation', 'Cell replacement', 'Row deletion']\n",
    "\n",
    "# Define the data for the baseline and novel proposed models for each category\n",
    "\n",
    "baseline_performance = 62.730\n",
    "our_method_performance = 69.130\n",
    "\n",
    "baseline_perturbed_performance = np.array([50.875, 57.804, 59.611, 56.256, 54.619])\n",
    "our_method_perturbed_performance = np.array([61.205, 67.380, 68.116, 66.096, 64.778])\n",
    "\n",
    "# baseline_data = list(baseline_performance - baseline_perturbed_performance)\n",
    "baseline_data = list(baseline_perturbed_performance - baseline_performance)\n",
    "# proposed_data = list(our_method_performance - our_method_perturbed_performance)\n",
    "proposed_data = list(our_method_perturbed_performance - our_method_performance)\n",
    "\n",
    "# Create a DataFrame for the data\n",
    "data = pd.DataFrame({'Category': categories, 'Baseline Model': baseline_data, 'Proposed Method': proposed_data})\n",
    "\n",
    "# Set a Seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create a color palette\n",
    "colors = sns.color_palette(\"deep\")\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "# Create a bar plot\n",
    "sns.barplot(x='Category', y='value', hue='variable', data=pd.melt(data, id_vars=['Category']), palette=colors, width=0.5)\n",
    "\n",
    "# Set the x-axis label and title\n",
    "# plt.xlabel('Perturbation Types', fontsize=12, loc=\"center\")\n",
    "plt.xlabel(\"Perturbation\", fontsize = 10, weight = \"bold\")\n",
    "plt.ylabel('Drop in Performance', fontsize=10, weight = \"bold\")\n",
    "# plt.title('Performance Comparison Across Perturbation Types', fontsize=12)\n",
    "\n",
    "# Customize legend\n",
    "plt.legend(title='Method', labels=['OmniTab', 'Proposed Method'], fontsize=10)\n",
    "\n",
    "legend = plt.gca().get_legend()\n",
    "legend.legend_handles[0].set_color(colors[0])\n",
    "legend.legend_handles[1].set_color(colors[1])\n",
    "\n",
    "# Add grid lines\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=15)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "designlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
